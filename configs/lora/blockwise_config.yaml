# Block-wise LoRA Configuration for DiffiT Architecture
# EXACT preservation of original LORA_CONFIG

lora:
  enabled: true
  alpha: 1.0  # LoRA scaling factor
  dropout: 0.0  # LoRA dropout for regularization
  
  # Target layers for LoRA adaptation
  targets:
    # TMSA (Temporal-Spatial Multi-head Self-Attention) components
    - "Wqs"  # Spatial attention projections
    - "Wks"
    - "Wvs"
    - "Wqt"  # Temporal attention projections 
    - "Wkt"
    - "Wvt"
    - "wo"   # Output projection
    - "WK"   # Bias projection in TMSA
    
    # MLP components
    - "linear_1"  # Feed-forward layers
    - "linear_2"
    
    # Additional linear layers in actual DiffiT
    - "time_embedding_mlp.1"  # Time embedding MLP layers
    - "time_embedding_mlp.3"
    - "embedding_layer"       # Label embedding (for latent model)
    - "linear_layer"          # Label embedding linear layer
    
    # Convolutional layers (treated as linear for LoRA purposes)
    - "conv3x3"  # Tokenizer and Head conv layers
    - "conv"     # Downsample/Upsample conv layers
    - "proj"     # Patch embedding projections
  
  include_WK: true  # Include WK bias projection in TMSA
  
  # Block-specific ranks optimized for actual DiffiT architecture
  encoder:
    default_rank: 8
    groups:
      1: 16  # Early encoder layers need higher adaptation capacity
      2: 12  # Intermediate layers
      3: 8   # Deeper layers
      4: 8   # Deepest encoder layers
  
  decoder:
    default_rank: 8
    groups:
      3: 8   # Deepest decoder layers
      2: 10  # Intermediate decoder layers
      1: 12  # Final decoder layers need good reconstruction
  
  ushape:
    default_rank: 8
    groups:
      1: 12  # First U-shaped group (skip connections important)
      2: 10  # Second U-shaped group
      3: 8   # Third U-shaped group
  
  latent:
    default_rank: 16  # Critical bottleneck needs high capacity
  
  time_embedding:
    default_rank: 4   # Time embeddings need less adaptation
  
  tokenizer:
    default_rank: 6   # Input tokenization
  
  head:
    default_rank: 8   # Output head
  
  default_rank: 8     # Fallback rank
  freeze_others: true # Freeze non-LoRA parameters
