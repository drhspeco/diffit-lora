# -*- coding: utf-8 -*-
"""diffit_blockwise_lora_finetuning (6).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MivWnxsWaEV29SGeC08xTKgXwfLUPAcy

# DiffiT with Block-wise LoRA Fine-tuning

This notebook demonstrates **state-of-the-art fine-tuning** of DiffiT (Diffusion Vision Transformers) using **Block-wise LoRA (Low-Rank Adaptation)** for efficient parameter adaptation.

## Key Features:
- **🎯 Block-wise LoRA**: Different ranks for different architectural components
- **📈 Transfer Learning**: Fine-tune from pretrained ImageNet models to CIFAR-10
- **💾 Memory Efficient**: Only train LoRA parameters (~1-5% of total parameters)
- **🔧 Flexible Configuration**: Easy-to-adjust LoRA settings per model component
- **📊 Comprehensive Evaluation**: FID, KID, LPIPS, and FLOPs metrics
- **🚀 Production Ready**: Best practices for deployment and model fusion

## What is Block-wise LoRA?
Instead of applying uniform LoRA ranks across all layers, **Block-wise LoRA** uses:
- **Higher ranks** for encoder layers (capture fine-grained features)
- **Medium ranks** for decoder layers (generate detailed outputs)  
- **Lower ranks** for latent blocks (efficient bottleneck processing)
- **Adaptive ranks** based on layer importance and computational constraints

## Fine-tuning Strategy:
1. **Freeze base model** weights (pretrained on large datasets)
2. **Inject LoRA adapters** with block-specific ranks
3. **Train only LoRA parameters** for domain adaptation
4. **Evaluate with comprehensive metrics**
5. **Fuse LoRA for deployment** (optional)

## 1. Environment Setup and Dependencies

Install required packages for Block-wise LoRA fine-tuning.

### 📦 Required Packages:
- `torch` - PyTorch deep learning framework
- `pytorch_lightning` - High-level PyTorch training framework  
- `datasets` - Hugging Face datasets library
- `torch-ema` - Exponential Moving Average for training
- `torchmetrics[image]` - Image quality metrics (FID, KID, etc.)
- `torchvision` - Computer vision utilities
- `numpy`, `matplotlib` - Numerical computing and visualization

### 🔧 Optional Packages:
- `lpips` - Learned Perceptual Image Patch Similarity
- `fvcore` - Facebook computer vision core library (for FLOPs)
- `wandb` - Weights & Biases experiment tracking
- `scipy`, `scikit-learn` - Additional scientific computing
"""

# packages with upgrade
## Install required packages (uncomment if needed)
# !pip install --upgrade datasets pytorch_lightning torch-ema torchmetrics[image] torch torchvision torchaudio
# !pip install --upgrade lpips fvcore  # For LPIPS and FLOPs
# !pip install wandb  # For experiment tracking (optional)
# !pip install --upgrade scipy scikit-learn  # For additional metrics
# !pip install --upgrade numpy matplotlib pillow tqdm spacy thinc fastai numba pandas numpy
pass

## Install required packages (uncomment if needed)
!pip install datasets pytorch_lightning torch-ema torchmetrics[image]
!pip install lpips fvcore  # For LPIPS and FLOPs
!pip install wandb  # For experiment tracking (optional)
!pip install scipy scikit-learn  # For additional metrics
!pip install numpy matplotlib pillow tqdm

# Install additional packages that might be missing
try:
    import datasets
    import pytorch_lightning as pl
    import torch_ema
    import torchmetrics
    print("✅ Core ML packages already installed")
except ImportError as e:
    print(f"⚠️ Missing package: {e}")
    print("Please run: pip install datasets pytorch_lightning torch-ema torchmetrics[image]")

# Check for optional dependencies
try:
    import lpips
    import fvcore
    print("✅ LPIPS and FLOPs packages available")
except ImportError:
    print("ℹ️ Optional: pip install lpips fvcore for advanced metrics")

try:
    import wandb
    print("✅ Weights & Biases available for experiment tracking")
except ImportError:
    print("ℹ️ Optional: pip install wandb for experiment tracking")

# Core imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import math
import numpy as np
import matplotlib.pyplot as plt
import random
import os
import re
import json
from datetime import datetime
from typing import Dict, List

# Deep learning utilities
from torchmetrics.image.fid import FrechetInceptionDistance
from torchmetrics.image.kid import KernelInceptionDistance
try:
    from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
    LPIPS_AVAILABLE = True
except ImportError:
    print("⚠️ LPIPS not available, install with: pip install lpips")
    LPIPS_AVAILABLE = False

from tqdm.auto import tqdm
from pytorch_lightning.callbacks import ModelCheckpoint
from torch_ema import ExponentialMovingAverage
from torch.utils.data import DataLoader

# Dataset and image processing
from datasets import load_dataset
from torchvision import transforms
from PIL import Image

print("✅ All packages imported successfully!")
print(f"🔥 PyTorch version: {torch.__version__}")
print(f"⚡ CUDA available: {torch.cuda.is_available()}")
print(f"🎯 LPIPS available: {LPIPS_AVAILABLE}")

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"🚀 Using device: {device}")

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
print("🎲 Random seeds set for reproducibility")

# Installation Commands (run if packages are missing)
installation_commands = [
    "pip install torch torchvision pytorch_lightning",
    "pip install datasets torch-ema torchmetrics[image]",
    "pip install numpy matplotlib pillow tqdm",
    "pip install scipy scikit-learn",  # For additional metrics
    "pip install lpips fvcore",        # Optional: advanced metrics
    "pip install wandb",               # Optional: experiment tracking
]

print("📦 Installation Commands (copy and run in terminal if needed):")
print("=" * 60)
for i, cmd in enumerate(installation_commands, 1):
    print(f"{i}. {cmd}")

print("\n" + "=" * 60)
print("💡 For Conda users, replace 'pip' with 'conda' where applicable")
print("🐍 For Python virtual environments: activate your environment first")

# Quick dependency check
def check_package(package_name, import_name=None):
    """Check if a package is available"""
    if import_name is None:
        import_name = package_name

    try:
        __import__(import_name)
        return True
    except ImportError:
        return False

print("\n🔍 Dependency Check:")
dependencies = [
    ("torch", "torch"),
    ("pytorch_lightning", "pytorch_lightning"),
    ("datasets", "datasets"),
    ("torch-ema", "torch_ema"),
    ("torchmetrics", "torchmetrics"),
    ("numpy", "numpy"),
    ("matplotlib", "matplotlib"),
    ("pillow", "PIL"),
    ("tqdm", "tqdm"),
]

optional_dependencies = [
    ("lpips", "lpips"),
    ("fvcore", "fvcore"),
    ("wandb", "wandb"),
    ("scipy", "scipy"),
    ("scikit-learn", "sklearn"),
]

missing_required = []
missing_optional = []

for package, import_name in dependencies:
    if check_package(package, import_name):
        print(f"   ✅ {package}")
    else:
        print(f"   ❌ {package}")
        missing_required.append(package)

print("\n🔧 Optional Dependencies:")
for package, import_name in optional_dependencies:
    if check_package(package, import_name):
        print(f"   ✅ {package}")
    else:
        print(f"   ⚪ {package} (optional)")
        missing_optional.append(package)

if missing_required:
    print(f"\n⚠️ Missing required packages: {', '.join(missing_required)}")
    print("Please install them before proceeding!")
else:
    print("\n🎉 All required dependencies are available!")

if missing_optional:
    print(f"ℹ️ Missing optional packages: {', '.join(missing_optional)}")
    print("Install them for additional features.")

"""## 2. Block-wise LoRA Implementation

Implement the core LoRA components with block-specific rank adaptation.
"""

class LoRALinear(nn.Module):
    """
    Low-Rank Adaptation for Linear layers
    Implements: output = W*x + (alpha/r)*(B@A*x)
    where W is frozen, A and B are trainable low-rank matrices
    """

    def __init__(self, base: nn.Linear, r: int, alpha: float = 1.0, dropout: float = 0.0):
        super().__init__()
        self.base = base
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r

        # Freeze base layer
        for param in self.base.parameters():
            param.requires_grad = False

        # LoRA parameters: A (input_dim x r), B (r x output_dim)
        self.lora_A = nn.Parameter(torch.randn(base.in_features, r) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(r, base.out_features))
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

        # For easy access to original dimensions
        self.in_features = base.in_features
        self.out_features = base.out_features

    def reset_parameters(self):
        """Initialize LoRA parameters"""
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

    def forward(self, x):
        """Forward pass: base_output + lora_adaptation"""
        base_out = self.base(x)

        # LoRA adaptation: x -> A -> dropout -> B -> scale
        lora_out = self.dropout(x @ self.lora_A) @ self.lora_B * self.scaling

        return base_out + lora_out

    def fuse(self):
        """Fuse LoRA weights into base layer for deployment"""
        with torch.no_grad():
            # Compute LoRA weight: (A @ B) * scaling
            # A: (in_features, r), B: (r, out_features) -> (in_features, out_features)
            lora_weight = (self.lora_A @ self.lora_B) * self.scaling
            # Add to base weight (which is (out_features, in_features))
            self.base.weight.data += lora_weight.T

        # Zero out LoRA parameters to avoid double counting
        self.lora_A.data.zero_()
        self.lora_B.data.zero_()

print("✅ LoRALinear implementation ready!")

def match_target_linear_name(name: str, targets: List[str]) -> bool:
    """Check if module name matches target linear layers"""
    last = name.split(".")[-1]
    # Also check for partial matches in the full path for complex modules
    return last in targets or any(target in name for target in targets)

def rank_for_module_path(module_path: str, cfg: Dict) -> int:
    """
    Determine LoRA rank based on module path and configuration
    Enhanced for actual DiffiT architecture with proper component recognition
    """

    # Time embedding components
    if "time_embedding" in module_path:
        return cfg.get("time_embedding", {}).get("default_rank", 4)

    # Tokenizer (input processing)
    if "tokenizer" in module_path:
        return cfg.get("tokenizer", {}).get("default_rank", 6)

    # Head (output processing)
    if "head" in module_path:
        return cfg.get("head", {}).get("default_rank", 8)

    # Encoder groups: diffit_res_block_group_1..4
    m = re.search(r"encoder\.diffit_res_block_group_(\d+)", module_path)
    if m:
        group_id = int(m.group(1))
        return cfg.get("encoder", {}).get("groups", {}).get(group_id,
               cfg.get("encoder", {}).get("default_rank", 8))

    # Decoder groups: diffit_res_block_group_1..3 (in decoder)
    m = re.search(r"decoder\.diffit_res_block_group_(\d+)", module_path)
    if m:
        group_id = int(m.group(1))
        return cfg.get("decoder", {}).get("groups", {}).get(group_id,
               cfg.get("decoder", {}).get("default_rank", 8))

    # U-Shaped root groups (for UShapedNetwork)
    m = re.search(r"(^|\.)diffit_res_block_group_(\d+)(\.|$)", module_path)
    if m:
        group_id = int(m.group(2))
        return cfg.get("ushape", {}).get("groups", {}).get(group_id,
               cfg.get("ushape", {}).get("default_rank", 8))

    # Latent block - critical for latent-space models
    if "latent_block" in module_path:
        return cfg.get("latent", {}).get("default_rank", 16)

    # Patch embedding and unpatch operations
    if "patch_embedding" in module_path or "unpatchify" in module_path:
        return cfg.get("latent", {}).get("default_rank", 12)

    # Label embedding (for latent-space models)
    if "label_embedding" in module_path:
        return cfg.get("time_embedding", {}).get("default_rank", 4)

    # Downsample/Upsample layers
    if "downsample" in module_path or "upsample" in module_path:
        return max(4, cfg.get("default_rank", 8) // 2)  # Lower rank for spatial operations

    # Fallback to default
    return cfg.get("default_rank", 8)

def inject_blockwise_lora(model: nn.Module, cfg: Dict):
    """
    Inject Block-wise LoRA into the actual DiffiT model
    Enhanced to handle real DiffiT architecture components

    Args:
        model: The DiffiT model to modify (UShapedNetwork or LatentDiffiTNetwork)
        cfg: Configuration dictionary with LoRA settings
    """
    targets = cfg.get("targets", ["Wqs", "Wks", "Wvs", "Wqt", "Wkt", "Wvt", "wo"])
    include_WK = cfg.get("include_WK", False)
    if include_WK and "WK" not in targets:
        targets.append("WK")

    alpha = cfg.get("alpha", 1.0)
    dropout = cfg.get("dropout", 0.0)

    replacements = []
    skipped = []

    def replace_in(parent: nn.Module, prefix: str):
        for child_name, child_module in parent.named_children():
            full_path = f"{prefix}.{child_name}" if prefix else child_name

            if isinstance(child_module, nn.Linear) and match_target_linear_name(child_name, targets):
                # Determine rank for this specific module
                rank = rank_for_module_path(full_path, cfg)

                # Skip very small layers or those with insufficient dimensions
                min_dim = min(child_module.in_features, child_module.out_features)
                if rank >= min_dim:
                    rank = max(1, min_dim // 2)  # Adjust rank to be feasible
                    if rank < 2:  # Skip if still too small
                        skipped.append((full_path, f"rank {rank} too small"))
                        continue

                try:
                    # Create LoRA wrapper
                    lora_module = LoRALinear(child_module, rank, alpha, dropout)
                    lora_module.reset_parameters()

                    # Replace the module
                    setattr(parent, child_name, lora_module)
                    replacements.append((full_path, rank))

                except Exception as e:
                    skipped.append((full_path, f"error: {e}"))

            else:
                # Recursively process children
                replace_in(child_module, full_path)

    replace_in(model, "")

    # Freeze non-LoRA parameters
    if cfg.get("freeze_others", True):
        for name, param in model.named_parameters():
            if "lora_" not in name:
                param.requires_grad = False

    print(f"✅ Block-wise LoRA injection complete!")
    print(f"📊 Successfully replaced {len(replacements)} linear layers:")
    for path, rank in replacements[:10]:  # Show first 10
        print(f"   • {path} -> rank {rank}")
    if len(replacements) > 10:
        print(f"   ... and {len(replacements) - 10} more")

    if skipped:
        print(f"⚠️ Skipped {len(skipped)} layers:")
        for path, reason in skipped[:5]:  # Show first 5 skipped
            print(f"   • {path}: {reason}")
        if len(skipped) > 5:
            print(f"   ... and {len(skipped) - 5} more skipped")

    return replacements

print("✅ Enhanced Block-wise LoRA injection utilities ready!")
print("🏗️ Optimized for actual DiffiT architecture (UShapedNetwork & LatentDiffiTNetwork)")

# Block-wise LoRA Configuration for Actual DiffiT Architecture
LORA_CONFIG = {
    "enabled": True,
    "alpha": 1.0,          # LoRA scaling factor
    "dropout": 0.0,        # LoRA dropout for regularization

    # Target layers for LoRA adaptation - updated for actual DiffiT architecture
    "targets": [
        # TMSA (Temporal-Spatial Multi-head Self-Attention) components
        "Wqs", "Wks", "Wvs",     # Spatial attention projections
        "Wqt", "Wkt", "Wvt",     # Temporal attention projections
        "wo",                     # Output projection
        "WK",                     # Bias projection in TMSA

        # MLP components
        "linear_1", "linear_2",   # Feed-forward layers

        # Additional linear layers in actual DiffiT
        "time_embedding_mlp.1",   # Time embedding MLP layers
        "time_embedding_mlp.3",
        "embedding_layer",        # Label embedding (for latent model)
        "linear_layer",           # Label embedding linear layer

        # Convolutional layers (treated as linear for LoRA purposes)
        "conv3x3",               # Tokenizer and Head conv layers
        "conv",                  # Downsample/Upsample conv layers
        "proj",                  # Patch embedding projections
    ],
    "include_WK": True,           # Include WK bias projection in TMSA

    # Block-specific ranks optimized for actual DiffiT architecture
    "encoder": {
        "default_rank": 8,
        "groups": {
            1: 16,  # Early encoder layers need higher adaptation capacity
            2: 12,  # Intermediate layers
            3: 8,   # Deeper layers
            4: 8    # Deepest encoder layers
        }
    },
    "decoder": {
        "default_rank": 8,
        "groups": {
            3: 8,   # Deepest decoder layers
            2: 10,  # Intermediate decoder layers
            1: 12   # Final decoder layers need good reconstruction
        }
    },
    "ushape": {
        "default_rank": 8,
        "groups": {
            1: 12,  # First U-shaped group (skip connections important)
            2: 10,  # Second U-shaped group
            3: 8    # Third U-shaped group
        }
    },
    "latent": {"default_rank": 16},       # Critical bottleneck needs high capacity
    "time_embedding": {"default_rank": 4}, # Time embeddings need less adaptation
    "tokenizer": {"default_rank": 6},     # Input tokenization
    "head": {"default_rank": 8},          # Output head

    "default_rank": 8,        # Fallback rank
    "freeze_others": True     # Freeze non-LoRA parameters
}

def calculate_lora_parameters(model: nn.Module) -> Dict:
    """Calculate LoRA parameter statistics"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    lora_params = sum(p.numel() for n, p in model.named_parameters()
                     if p.requires_grad and "lora_" in n)

    return {
        "total_parameters": total_params,
        "trainable_parameters": trainable_params,
        "lora_parameters": lora_params,
        "trainable_ratio": trainable_params / total_params * 100,
        "lora_ratio": lora_params / total_params * 100
    }

def save_lora_weights(model: nn.Module, path: str):
    """Save only LoRA parameters"""
    state = {}
    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            state[f"{name}.lora_A"] = module.lora_A.data
            state[f"{name}.lora_B"] = module.lora_B.data
    torch.save(state, path)
    print(f"💾 LoRA weights saved to {path}")

def load_lora_weights(model: nn.Module, path: str, strict: bool = False):
    """Load LoRA parameters"""
    ckpt = torch.load(path, map_location="cpu")
    misses = []

    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            if f"{name}.lora_A" in ckpt:
                module.lora_A.data = ckpt[f"{name}.lora_A"]
            else:
                misses.append(f"{name}.lora_A")

            if f"{name}.lora_B" in ckpt:
                module.lora_B.data = ckpt[f"{name}.lora_B"]
            else:
                misses.append(f"{name}.lora_B")

    if strict and misses:
        raise RuntimeError(f"Missing LoRA weights: {misses}")
    elif misses:
        print(f"⚠️ Warning: Missing LoRA weights: {len(misses)} parameters")

    print(f"📥 LoRA weights loaded from {path}")

def fuse_all_lora(model: nn.Module):
    """Fuse all LoRA weights into base model for deployment"""
    fused_count = 0
    for module in model.modules():
        if isinstance(module, LoRALinear):
            module.fuse()
            fused_count += 1
    print(f"🔗 Fused {fused_count} LoRA modules into base weights")

print("✅ LoRA configuration updated for actual DiffiT architecture!")
print(f"🎯 Enhanced target layers: {len(LORA_CONFIG['targets'])} layer types")
print(f"🏗️ Block-specific ranks configured for encoder, decoder, and U-shape")

"""## 3. DiffiT Model Architecture

Import the core DiffiT components (simplified for this fine-tuning focused notebook).
"""

# Import DiffiT components from the actual implementation
import sys
import os
from pathlib import Path

# Setup paths for importing DiffiT architecture
current_dir = Path.cwd()
parent_dir = current_dir.parent if current_dir.name == "codebase" else current_dir
diffit_file = current_dir / "diffit_image_space_architecture.py"

# Check if the DiffiT architecture file exists
if not diffit_file.exists():
    print(f"⚠️ DiffiT architecture file not found at: {diffit_file}")
    print("Creating a minimal version of required components...")

    # Create a minimal diffit_image_space_architecture.py if it doesn't exist
    minimal_diffit_code = '''
# Minimal DiffiT architecture for LoRA fine-tuning
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import math

# Add the minimal required components here
# Note: For production use, copy the full diffit_image_space_architecture.py file
    '''

    with open(diffit_file, 'w') as f:
        f.write(minimal_diffit_code)
    print(f"📝 Created minimal architecture file at: {diffit_file}")

# Add current directory to Python path
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

try:
    # Import the actual DiffiT architecture components
    from diffit_image_space_architecture import (
        UShapedNetwork,
        LatentDiffiTNetwork,
        causal_mask,
        extract,
        linear_beta_schedule,
        q_sample,
        p_losses,
        sample,
        p_sample_loop,
        LayerNormalization,
        MLP,
        TMSA,
        DiffiTBlock,
        DiffiTResBlock,
        ResBlockGroup,
        Tokenizer,
        Head,
        SinusoidalPositionEmbeddings,
        TimeEmbedding,
        DiffiTEncoder,
        DiffiTDecoder,
        Downsample,
        Upsample,
        PatchEmbedding,
        Unpatch,
        LatentDiffiTTransformerBlock,
        LabelEmbedding
    )

    print("✅ Successfully imported actual DiffiT architecture components!")
    print("🏗️ Available models: UShapedNetwork, LatentDiffiTNetwork")
    print("🔧 Core components: TMSA, DiffiTBlock, ResBlockGroup, etc.")
    DIFFIT_AVAILABLE = True

except ImportError as e:
    print(f"❌ Failed to import DiffiT components: {e}")
    print(f"📁 Please ensure 'diffit_image_space_architecture.py' exists in: {current_dir}")
    print("🔧 Alternative: Copy the file from the main repository or use the simplified version")
    DIFFIT_AVAILABLE = False

    # Fallback: Define minimal placeholder classes
    print("🔄 Using fallback placeholder classes...")

    class UShapedNetwork(pl.LightningModule):
        def __init__(self, **kwargs):
            super().__init__()
            print("⚠️ Using placeholder UShapedNetwork - replace with actual implementation")

        def forward(self, x, t, l=None):
            return torch.zeros_like(x)

    class LatentDiffiTNetwork(pl.LightningModule):
        def __init__(self, **kwargs):
            super().__init__()
            print("⚠️ Using placeholder LatentDiffiTNetwork - replace with actual implementation")

        def forward(self, x, t, l=None):
            return torch.zeros_like(x)

print(f"🎯 DiffiT architecture available: {DIFFIT_AVAILABLE}")
if not DIFFIT_AVAILABLE:
    print("💡 To get the full implementation:")
    print("   1. Copy 'diffit_image_space_architecture.py' to this directory")
    print("   2. Or run the original DiffiT notebook to generate the file")
    print("   3. Or download from the repository")

# Note: Core DiffiT components are now imported from diffit_image_space_architecture.py
# This includes: LayerNormalization, MLP, TMSA, DiffiTBlock, TimeEmbedding, etc.

# Additional utility functions specific to this fine-tuning implementation
def get_model_config_for_architecture(model_type: str, base_config: dict) -> dict:
    """
    Get model configuration based on architecture type
    """
    if model_type == "image-space":
        return {
            "learning_rate": base_config["learning_rate"],
            "d_model": base_config["d_model"],
            "num_heads": base_config["num_heads"],
            "dropout": base_config["dropout_prob"],
            "d_ff": base_config["d_ff"],
            "img_size": base_config["img_size"],
            "device": device,
            "denoising_steps": base_config["denoising_steps"],
            # U-shaped network specific parameters
            "L1": 2,  # ResBlock group depths
            "L2": 2,
            "L3": 2,
            "L4": 2,
        }
    elif model_type == "latent-space":
        return {
            "N_latent_blocks": 1,
            "img_size": base_config["img_size"],
            "d_model": base_config["d_model"],
            "num_heads": base_config["num_heads"],
            "dropout": base_config["dropout_prob"],
            "d_ff": base_config["d_ff"],
            "learning_rate": base_config["learning_rate"],
            "denoising_steps": base_config["denoising_steps"],
            "device": device,
            "label_size": 10,  # CIFAR-10 has 10 classes
        }
    else:
        raise ValueError(f"Unknown model type: {model_type}")

print("✅ Model configuration utilities ready!")
print("🏗️ Real DiffiT components imported and ready to use!")

# Setup DiffiT Architecture File
def setup_diffit_architecture():
    """
    Setup the DiffiT architecture file for import
    This will either copy the existing file or create it from the original notebook
    """
    current_dir = Path.cwd()
    diffit_file = current_dir / "diffit_image_space_architecture.py"

    if diffit_file.exists():
        print(f"✅ DiffiT architecture file found: {diffit_file}")
        return True

    # Try to find the file in parent directories or common locations
    search_paths = [
        current_dir.parent / "diffit_image_space_architecture.py",
        current_dir / "../diffit_image_space_architecture.py",
        Path("./diffit_image_space_architecture.py"),
    ]

    for path in search_paths:
        if Path(path).exists():
            print(f"📋 Copying DiffiT architecture from: {path}")
            import shutil
            shutil.copy2(path, diffit_file)
            print(f"✅ DiffiT architecture copied to: {diffit_file}")
            return True

    # If not found, provide instructions
    print(f"⚠️ DiffiT architecture file not found in any of these locations:")
    for path in search_paths:
        print(f"   • {path}")

    print("\n💡 To setup the DiffiT architecture file:")
    print("   1. Run the original 'diffit_image_space_architecture.ipynb' notebook")
    print("   2. Or copy the .py version from your project")
    print("   3. Or download from the repository")
    print(f"   4. Place it in: {current_dir}")

    return False

# Check if we have Google Colab specific setup
IN_COLAB = 'google.colab' in sys.modules

if IN_COLAB:
    print("🌟 Running in Google Colab")
    # Colab-specific setup
    try:
        from google.colab import drive
        print("📁 Mounting Google Drive...")
        drive.mount('/content/drive')

        # Try to find DiffiT file in Drive
        drive_paths = [
            "/content/drive/MyDrive/diffit_image_space_architecture.py",
            "/content/drive/MyDrive/tmu-thesis/codebase/diffit_image_space_architecture.py",
        ]

        for drive_path in drive_paths:
            if Path(drive_path).exists():
                print(f"📋 Copying from Google Drive: {drive_path}")
                import shutil
                shutil.copy2(drive_path, "./diffit_image_space_architecture.py")
                break

    except ImportError:
        print("⚠️ Google Colab drive mount not available")

# Setup the architecture file
architecture_ready = setup_diffit_architecture()
print(f"🎯 DiffiT architecture setup: {'✅ Ready' if architecture_ready else '❌ Needs manual setup'}")

"""## 4. Fine-tuning Configuration and Setup

Configure the fine-tuning experiment with optimal hyperparameters.
"""

# Fine-tuning Configuration (FIXED - Lower Learning Rate)
FINE_TUNE_CONFIG = {
    # Model architecture
    "model_type": "image-space",  # "image-space" or "latent-space"
    "img_size": 32,
    "d_model": 128,
    "num_heads": 2,
    "dropout_prob": 0.0,
    "d_ff": 256,
    "denoising_steps": 500,
    # Dataset settings
    "dataset_name": "CIFAR",  # Target domain for fine-tuning
    "batch_size_train": 32,  # Smaller batch for fine-tuning
    "batch_size_test": 16,
    "num_workers": 2,
    # Fine-tuning hyperparameters (FIXED)
    "learning_rate": 5e-4,  # FIXED: Much lower LR to prevent overflow (was 1e-1)
    "num_epochs": 5,  # Fewer epochs than pre-training
    "warmup_steps": 100,  # Learning rate warmup
    "weight_decay": 1e-5,  # Light regularization
    # Pretrained model paths
    "pretrained_path": "best_model.ckpt",  # Base model
    "output_dir": "./weights/lora_finetuned/",  # LoRA checkpoints
    "experiment_name": f"diffit_lora_cifar_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    # Transfer learning settings
    "freeze_base_model": True,
    "enable_ema": True,  # Exponential moving average
    "gradient_clip_val": 0.5,  # FIXED: More conservative gradient clipping (was 1.0)
    # Evaluation settings
    "eval_every_n_epochs": 2,
    "save_top_k": 3,  # Save best 3 checkpoints
    "monitor_metric": "val_loss",
}

# Create output directories
os.makedirs(FINE_TUNE_CONFIG["output_dir"], exist_ok=True)
os.makedirs(f"{FINE_TUNE_CONFIG['output_dir']}/logs", exist_ok=True)

print("🎯 Fine-tuning Configuration (FIXED):")
print(f"   📊 Model: {FINE_TUNE_CONFIG['model_type']}")
print(f"   🎨 Dataset: {FINE_TUNE_CONFIG['dataset_name']}")
print(f"   📈 Learning Rate: {FINE_TUNE_CONFIG['learning_rate']} (FIXED - was 0.1)")
print(f"   🔄 Epochs: {FINE_TUNE_CONFIG['num_epochs']}")
print(f"   ✂️ Gradient Clipping: {FINE_TUNE_CONFIG['gradient_clip_val']} (FIXED - was 1.0)")
print(f"   💾 Output: {FINE_TUNE_CONFIG['output_dir']}")
print(f"   🏷️ Experiment: {FINE_TUNE_CONFIG['experiment_name']}")

# Global tracking variables
train_losses = []
val_losses = []
lora_stats = {}

print("✅ Fine-tuning configuration ready with overflow fixes!")
print("🔧 Key fixes:")
print("   • Learning rate: 1e-1 → 5e-4 (200x smaller)")
print("   • Gradient clipping: 1.0 → 0.5 (more conservative)")
print("   • Training setup will use FP32 instead of FP16")

"""## 5. Dataset Loading and Preprocessing

Load target dataset for fine-tuning with optimal preprocessing.
"""

def get_cifar_dataset():
    """Load CIFAR-10 dataset with fine-tuning optimized preprocessing"""
    dataset = load_dataset("cifar10")

    # Enhanced preprocessing for fine-tuning
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1] range
    ])

    # Data augmentation for training (helps with fine-tuning)
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=5),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    train_data = []
    val_data = []

    # Process training data with augmentation
    for item in dataset["train"]:
        img_tensor = train_transform(item["img"])
        train_data.append(img_tensor)

    # Process test data (validation) without augmentation
    for item in dataset["test"]:
        img_tensor = transform(item["img"])
        val_data.append(img_tensor)

    # Split train data for validation if needed
    train_size = int(0.9 * len(train_data))
    val_size = len(train_data) - train_size

    if val_size > 0:
        train_split, val_split = torch.utils.data.random_split(
            train_data, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )
        train_data = list(train_split)
        val_data.extend(list(val_split))

    return train_data, val_data

def create_dataloaders(train_data, val_data, config):
    """Create optimized dataloaders for fine-tuning"""

    train_loader = DataLoader(
        train_data,
        batch_size=config["batch_size_train"],
        shuffle=True,
        num_workers=config["num_workers"],
        pin_memory=True,        # Faster GPU transfer
        drop_last=True,         # Consistent batch sizes
        persistent_workers=True if config["num_workers"] > 0 else False
    )

    val_loader = DataLoader(
        val_data,
        batch_size=config["batch_size_test"],
        shuffle=False,
        num_workers=config["num_workers"],
        pin_memory=True,
        persistent_workers=True if config["num_workers"] > 0 else False
    )

    return train_loader, val_loader

# Load dataset
print("📥 Loading CIFAR-10 dataset...")
train_data, val_data = get_cifar_dataset()

print(f"✅ Dataset loaded successfully!")
print(f"   📊 Training samples: {len(train_data)}")
print(f"   📊 Validation samples: {len(val_data)}")
print(f"   🎨 Image shape: {train_data[0].shape}")
print(f"   📏 Value range: [{train_data[0].min():.2f}, {train_data[0].max():.2f}]")

# Create dataloaders
train_loader, val_loader = create_dataloaders(train_data, val_data, FINE_TUNE_CONFIG)

print(f"🔄 DataLoaders created:")
print(f"   📊 Training batches: {len(train_loader)}")
print(f"   📊 Validation batches: {len(val_loader)}")
print(f"   📦 Train batch size: {FINE_TUNE_CONFIG['batch_size_train']}")
print(f"   📦 Val batch size: {FINE_TUNE_CONFIG['batch_size_test']}")

"""## 6. Actual DiffiT Model with LoRA Injection

**🎯 Key Changes from Simplified Model:**

### Architecture Improvements:
- **✅ Real TMSA**: Temporal-Spatial Multi-head Self-Attention with spatial (xs) and temporal (xt) components
- **✅ U-shaped Architecture**: Proper encoder-decoder with skip connections for diffusion models
- **✅ Convolutional Components**: Conv2D tokenizer, Group normalization, proper image processing pipeline
- **✅ ResBlock Groups**: Hierarchical block organization with configurable depths (L1, L2, L3, L4)
- **✅ Downsampling/Upsampling**: Proper spatial resolution handling for multi-scale processing

### LoRA Integration Benefits:
- **🎯 Block-wise Ranks**: Different adaptation capacities for encoder (16→8), decoder (8→12), and latent (16) blocks
- **🔧 Component-specific**: Optimized ranks for TMSA, MLP, time embeddings, and spatial operations
- **💾 Memory Efficient**: Only ~1-5% of parameters are trainable while maintaining full model expressiveness
- **⚡ Training Speed**: Faster convergence on domain adaptation tasks

### Supported Architectures:
1. **UShapedNetwork** (Image-space): Direct image-to-image diffusion with U-Net-like structure
2. **LatentDiffiTNetwork** (Latent-space): Encoder→Latent→Decoder with patch embeddings and transformer blocks
"""

# Use actual DiffiT architecture instead of simplified model
def create_diffit_model(config):
    """
    Create the actual DiffiT model based on configuration

    Args:
        config: Configuration dictionary

    Returns:
        Configured DiffiT model (UShapedNetwork or LatentDiffiTNetwork)
    """
    model_type = config["model_type"]
    model_config = get_model_config_for_architecture(model_type, config)

    if model_type == "image-space":
        # Use the actual U-shaped DiffiT architecture
        model = UShapedNetwork(
            learning_rate=model_config["learning_rate"],
            d_model=model_config["d_model"],
            num_heads=model_config["num_heads"],
            dropout=model_config["dropout"],
            d_ff=model_config["d_ff"],
            img_size=model_config["img_size"],
            device=model_config["device"],
            denoising_steps=model_config["denoising_steps"],
            L1=model_config["L1"],
            L2=model_config["L2"],
            L3=model_config["L3"],
            L4=model_config["L4"]
        )
        print("🏗️ Created UShapedNetwork (Image-space DiffiT)")

    elif model_type == "latent-space":
        # Use the actual Latent DiffiT architecture
        model = LatentDiffiTNetwork(
            N_latent_blocks=model_config["N_latent_blocks"],
            img_size=model_config["img_size"],
            d_model=model_config["d_model"],
            num_heads=model_config["num_heads"],
            dropout=model_config["dropout"],
            d_ff=model_config["d_ff"],
            learning_rate=model_config["learning_rate"],
            denoising_steps=model_config["denoising_steps"],
            device=model_config["device"],
            label_size=model_config["label_size"]
        )
        print("🏗️ Created LatentDiffiTNetwork (Latent-space DiffiT)")

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Move model to device
    model = model.to(device)

    # Print architecture info
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"📊 Model Architecture Details:")
    print(f"   • Type: {model_type}")
    print(f"   • Total parameters: {total_params:,}")
    print(f"   • Trainable parameters: {trainable_params:,}")
    print(f"   • Model dimension: {model_config['d_model']}")
    print(f"   • Number of heads: {model_config['num_heads']}")
    print(f"   • Image size: {model_config['img_size']}x{model_config['img_size']}")
    print(f"   • Denoising steps: {model_config['denoising_steps']}")

    return model

# Initialize the actual DiffiT model
print("🚀 Initializing actual DiffiT model...")
model = create_diffit_model(FINE_TUNE_CONFIG)

# Load pretrained weights if available
pretrained_path = FINE_TUNE_CONFIG["pretrained_path"]
if os.path.exists(pretrained_path):
    print(f"📥 Loading pretrained weights from {pretrained_path}")
    try:
        # For PyTorch Lightning models, use load_from_checkpoint
        if FINE_TUNE_CONFIG["model_type"] == "image-space":
            pretrained_model = UShapedNetwork.load_from_checkpoint(
                pretrained_path,
                **get_model_config_for_architecture("image-space", FINE_TUNE_CONFIG)
            )
        else:
            pretrained_model = LatentDiffiTNetwork.load_from_checkpoint(
                pretrained_path,
                **get_model_config_for_architecture("latent-space", FINE_TUNE_CONFIG)
            )

        # Load state dict
        model.load_state_dict(pretrained_model.state_dict(), strict=False)
        print("✅ Pretrained weights loaded successfully!")

    except Exception as e:
        print(f"⚠️ Could not load pretrained weights: {e}")
        print("� Continuing with randomly initialized weights")
else:
    print("⚠️ No pretrained weights found, starting from scratch")
    print(f"💡 Expected path: {pretrained_path}")

print("✅ Actual DiffiT model initialized successfully!")
print(f"🎯 Ready for LoRA injection and fine-tuning!")

# Device Management Utilities for LoRA Fine-tuning
def fix_lora_device_mismatch(model, target_device=None):
    """
    Fix device mismatches in LoRA modules

    Args:
        model: Model with LoRA modules
        target_device: Device to move to (auto-detect if None)
    """
    if target_device is None:
        target_device = next(model.parameters()).device

    print(f"🔧 Fixing LoRA device mismatches, target device: {target_device}")

    fixed_modules = 0
    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            # Check if any component is on wrong device
            if (module.lora_A.device != target_device or
                module.lora_B.device != target_device or
                module.base.weight.device != target_device):

                # FIXED: Use .data to avoid Parameter assignment issues
                # and move the module itself to preserve Parameter types
                module.to(target_device)

                # Alternatively, move tensors properly while preserving Parameter type
                if module.lora_A.device != target_device:
                    module.lora_A.data = module.lora_A.data.to(target_device)
                if module.lora_B.device != target_device:
                    module.lora_B.data = module.lora_B.data.to(target_device)

                # Base layer should be moved via the module's .to() method
                if hasattr(module, 'base'):
                    module.base = module.base.to(target_device)

                fixed_modules += 1

    print(f"✅ Fixed {fixed_modules} LoRA modules")
    return fixed_modules

def verify_model_device_consistency(model):
    """
    Verify all model components are on the same device

    Args:
        model: Model to check

    Returns:
        bool: True if consistent, False otherwise
    """
    model_device = next(model.parameters()).device
    issues = []

    for name, param in model.named_parameters():
        if param.device != model_device:
            issues.append(f"{name}: {param.device} != {model_device}")

    if issues:
        print(f"❌ Device inconsistencies found:")
        for issue in issues[:5]:  # Show first 5
            print(f"   • {issue}")
        if len(issues) > 5:
            print(f"   ... and {len(issues) - 5} more issues")
        return False
    else:
        print(f"✅ All parameters on device: {model_device}")
        return True

print("🛠️ Device management utilities loaded!")

# Apply blockwise LoRA injection to the actual DiffiT model
print("🔄 Injecting blockwise LoRA adapters into actual DiffiT architecture...")

# Count original parameters
original_params = sum(p.numel() for p in model.parameters())
original_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"📊 Before LoRA injection:")
print(f"   • Total parameters: {original_params:,}")
print(f"   • Trainable parameters: {original_trainable:,}")

# Inject LoRA into the actual DiffiT model
try:
    replacements = inject_blockwise_lora(model, LORA_CONFIG)

    # CRITICAL FIX: Ensure all LoRA components are on the correct device
    model_device = next(model.parameters()).device
    print(f"🔧 Ensuring all LoRA components are on device: {model_device}")

    # Move all LoRA modules to the correct device using proper Parameter handling
    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            # FIXED: Use .data to modify parameter tensors or move the entire module
            # Move the entire module to preserve Parameter types
            module.to(model_device)

            # Double-check that individual components are on the right device
            if module.lora_A.device != model_device:
                module.lora_A.data = module.lora_A.data.to(model_device)
            if module.lora_B.device != model_device:
                module.lora_B.data = module.lora_B.data.to(model_device)

    print(f"✅ All LoRA components moved to device: {model_device}")

    # Count LoRA parameters
    lora_stats = calculate_lora_parameters(model)

    print(f"\n🎯 After LoRA injection:")
    print(f"   • Total parameters: {lora_stats['total_parameters']:,}")
    print(f"   • LoRA parameters: {lora_stats['lora_parameters']:,}")
    print(f"   • Trainable parameters: {lora_stats['trainable_parameters']:,}")
    print(f"   • Parameter efficiency: {lora_stats['lora_ratio']:.2f}%")
    print(f"   • Trainable efficiency: {lora_stats['trainable_ratio']:.2f}%")

    # Verify LoRA injection by examining modules
    print(f"\n🔍 LoRA module verification:")
    lora_modules = {}
    total_lora_modules = 0

    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            total_lora_modules += 1
            lora_modules[name] = {
                "rank": module.r,
                "original_shape": module.base.weight.shape,
                "lora_A_shape": module.lora_A.shape,
                "lora_B_shape": module.lora_B.shape,
                "scaling": module.scaling,
                "device_A": str(module.lora_A.device),
                "device_B": str(module.lora_B.device),
                "device_base": str(module.base.weight.device)
            }

    print(f"✅ Successfully injected LoRA into {total_lora_modules} modules")

    # Show details for some key modules
    key_modules = [name for name in lora_modules.keys()
                   if any(key in name for key in ["tmsa", "attention", "mlp", "linear"])][:8]

    if key_modules:
        print(f"\n📋 Key LoRA modules (showing {len(key_modules)}):")
        for name in key_modules:
            info = lora_modules[name]
            print(f"   • {name}:")
            print(f"     - Rank: {info['rank']}")
            print(f"     - Original: {info['original_shape']}")
            print(f"     - LoRA A: {info['lora_A_shape']}, B: {info['lora_B_shape']}")
            print(f"     - Scaling: {info['scaling']:.3f}")
            print(f"     - Devices: A={info['device_A']}, B={info['device_B']}, Base={info['device_base']}")

    # Verify device consistency before testing
    print(f"\n🔍 Device consistency check:")
    device_issues = 0
    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            if (module.lora_A.device != model_device or
                module.lora_B.device != model_device or
                module.base.weight.device != model_device):
                print(f"❌ Device mismatch in {name}")
                device_issues += 1

    if device_issues == 0:
        print(f"✅ All LoRA components are on the correct device: {model_device}")
    else:
        print(f"❌ Found {device_issues} device mismatches!")
        raise RuntimeError("Device mismatch detected in LoRA components")

    # Test forward pass to ensure model still works
    print(f"\n🧪 Testing forward pass...")
    test_batch_size = 2
    test_input = torch.randn(test_batch_size, 3, FINE_TUNE_CONFIG["img_size"], FINE_TUNE_CONFIG["img_size"]).to(model_device)
    test_timesteps = torch.randint(0, FINE_TUNE_CONFIG["denoising_steps"], (test_batch_size,)).long().to(model_device)

    model.eval()
    with torch.no_grad():
        if isinstance(model, LatentDiffiTNetwork):
            # Latent model needs label input
            test_labels = torch.randint(0, 10, (test_batch_size,)).long().to(model_device)
            test_output = model(test_input, test_timesteps, test_labels)
        else:
            # U-shaped model
            test_output = model(test_input, test_timesteps)

    print(f"✅ Forward pass successful!")
    print(f"   • Input shape: {test_input.shape}")
    print(f"   • Output shape: {test_output.shape}")
    print(f"   • Output device: {test_output.device}")
    print(f"   • Model type: {type(model).__name__}")

    model.train()  # Set back to training mode

except Exception as e:
    print(f"❌ Error during LoRA injection: {e}")
    print(f"🔧 This might be due to Parameter assignment issue or architecture mismatch.")
    # Print debugging information
    print(f"\n🔍 Model device info:")
    try:
        for name, param in list(model.named_parameters())[:5]:
            print(f"   • {name}: {param.device} ({type(param).__name__})")
    except:
        pass

    # Try to use the fix utility if available
    try:
        print(f"\n🔧 Attempting automatic device fix...")
        fix_lora_device_mismatch(model)
        print(f"✅ Device fix completed, retrying forward pass...")

        # Retry forward pass
        test_batch_size = 2
        model_device = next(model.parameters()).device
        test_input = torch.randn(test_batch_size, 3, FINE_TUNE_CONFIG["img_size"], FINE_TUNE_CONFIG["img_size"]).to(model_device)
        test_timesteps = torch.randint(0, FINE_TUNE_CONFIG["denoising_steps"], (test_batch_size,)).long().to(model_device)

        model.eval()
        with torch.no_grad():
            if isinstance(model, LatentDiffiTNetwork):
                test_labels = torch.randint(0, 10, (test_batch_size,)).long().to(model_device)
                test_output = model(test_input, test_timesteps, test_labels)
            else:
                test_output = model(test_input, test_timesteps)

        print(f"✅ Forward pass successful after fix!")
        print(f"   • Output shape: {test_output.shape}")
        print(f"   • Output device: {test_output.device}")

        model.train()

    except Exception as fix_error:
        print(f"❌ Automatic fix failed: {fix_error}")
        raise e

print(f"\n🎉 LoRA injection completed successfully!")
print(f"🚀 Model ready for fine-tuning with {lora_stats['lora_parameters']:,} LoRA parameters!")

"""## 7. Training Loop with LoRA

Now we'll implement the training loop that only updates LoRA parameters while keeping the pretrained weights frozen.
"""

# Setup PyTorch Lightning trainer with FP16 overflow fix
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger

# Callbacks
checkpoint_callback = ModelCheckpoint(
    dirpath=FINE_TUNE_CONFIG["output_dir"],
    filename="lora-diffit-{epoch:02d}-{val_loss:.2f}",
    monitor="val_loss",
    mode="min",
    save_top_k=3,
    save_last=True
)

early_stopping = EarlyStopping(
    monitor="val_loss",
    patience=10,
    mode="min",
    verbose=True
)

lr_monitor = LearningRateMonitor(logging_interval="step")

# Logger
logger = TensorBoardLogger(
    save_dir=f"{FINE_TUNE_CONFIG['output_dir']}/logs",
    name=FINE_TUNE_CONFIG["experiment_name"],
    version=None
)

# FIXED: Use FP32 instead of FP16 to avoid overflow issues
# Mixed precision can cause overflow with LoRA fine-tuning
trainer = pl.Trainer(
    max_epochs=FINE_TUNE_CONFIG["num_epochs"],
    accelerator="auto",
    devices="auto",
    precision="32-true",  # Fixed: Use FP32 to avoid overflow
    gradient_clip_val=FINE_TUNE_CONFIG["gradient_clip_val"],
    callbacks=[checkpoint_callback, early_stopping, lr_monitor],
    logger=logger,
    enable_checkpointing=True,
    enable_progress_bar=True,
    enable_model_summary=True,
    check_val_every_n_epoch=FINE_TUNE_CONFIG["eval_every_n_epochs"],
    # Additional stability settings
    detect_anomaly=False,  # Disable anomaly detection for speed
    log_every_n_steps=50,  # Reduce logging frequency
)

print("✅ Training setup complete!")
print(f"🎯 Training for {FINE_TUNE_CONFIG['num_epochs']} epochs")
print(f"💾 Checkpoints will be saved to: {FINE_TUNE_CONFIG['output_dir']}")
print(f"📊 Monitoring metric: {FINE_TUNE_CONFIG['monitor_metric']}")
print(f"🔧 Using FP32 precision to avoid overflow issues")

# Print model summary
print(f"\n📊 Model Summary:")
print(f"  • Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"  • Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
print(f"  • LoRA parameters: {lora_stats['lora_parameters']:,}")
print(f"  • Parameter efficiency: {lora_stats['lora_ratio']:.2f}%")

# Verify only LoRA parameters are trainable
trainable_params = [(n, p) for n, p in model.named_parameters() if p.requires_grad]
print(f"\n🔒 Trainable parameters ({len(trainable_params)}):")
for name, param in trainable_params[:10]:  # Show first 10
    print(f"  • {name}: {param.shape}")
if len(trainable_params) > 10:
    print(f"  ... and {len(trainable_params) - 10} more parameters")

# Fit the model (using correct dataloader variable names)
try:
    trainer.fit(model, train_loader, val_loader)  # Fixed: using correct variable names
    print("✅ Training completed successfully!")
except KeyboardInterrupt:
    print("⏹️ Training interrupted by user")
except Exception as e:
    print(f"❌ Training failed with error: {e}")

# Save final LoRA weights
final_lora_path = os.path.join(FINE_TUNE_CONFIG["output_dir"], "final_lora_weights.pth")
save_lora_weights(model, final_lora_path)
print(f"💾 Final LoRA weights saved to: {final_lora_path}")

"""## 8. Comprehensive Evaluation (Fixed)

Evaluate the fine-tuned model with multiple metrics including FID, KID, LPIPS, and computational cost.

**Fixed Issues:**
- ✅ LPIPS normalization: Convert from [-1,1] to [0,1] range properly
- ✅ FLOPs calculation: Fixed import issues and device management
- ✅ NaN handling: Skip samples with invalid values
- ✅ Memory management: Move tensors appropriately between devices
"""

# Additional imports for comprehensive evaluation (same as base DiffiT notebook)
try:
    from torchmetrics.image import FrechetInceptionDistance
    from torchmetrics.image.kid import KernelInceptionDistance
    from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
    import lpips
    # Updated imports for FLOPs calculation
    try:
        from fvcore.nn import flop_count_table, FlopCountAnalysis
    except ImportError:
        # Alternative method for FLOPs calculation
        try:
            from torchprofile import profile_macs
        except ImportError:
            profile_macs = None
        FlopCountAnalysis = None
    print("All evaluation metrics imported successfully!")
except ImportError as e:
    print(f"Some evaluation packages need to be installed: {e}")
    print("Run: pip install lpips torchprofile fvcore")

def get_real_features(dataset, batch_size):
    """Sample real images for metric computation"""
    items = random.sample(dataset, batch_size)
    real_features = torch.stack(items)
    return real_features

def calculate_comprehensive_metrics(real_images, generated_images, model):
    """Calculate FID, KID, LPIPS, and FLOPs metrics (fixed version)"""
    metrics_results = {}

    print("Calculating comprehensive evaluation metrics...")

    # Ensure proper tensor format and device placement
    if isinstance(real_images, list):
        real_images = torch.stack(real_images)
    if isinstance(generated_images, list):
        generated_images = torch.stack(generated_images)

    # Move to CPU for metrics calculation to avoid device issues
    real_images = real_images.cuda()
    generated_images = generated_images.cuda()

    # Convert to proper format for metrics (uint8, 0-255 range)
    real_images_uint8 = ((real_images + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)
    gen_images_uint8 = ((generated_images + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)

    # 1. FID Score
    print("Computing FID...")
    try:
        fid = FrechetInceptionDistance(feature=64, normalize=True)
        fid.update(real_images_uint8, real=True)
        fid.update(gen_images_uint8, real=False)
        fid_score = fid.compute()
        metrics_results['FID'] = fid_score.item()
    except Exception as e:
        print(f"FID calculation failed: {e}")
        metrics_results['FID'] = None

    # 2. KID Score
    print("Computing KID...")
    try:
        # Use smaller subset size to avoid memory issues
        subset_size = min(50, len(real_images), len(generated_images))
        kid = KernelInceptionDistance(feature=64, subset_size=subset_size, normalize=True)
        kid.update(real_images_uint8[:subset_size], real=True)
        kid.update(gen_images_uint8[:subset_size], real=False)
        kid_mean, kid_std = kid.compute()
        metrics_results['KID_mean'] = kid_mean.item()
        metrics_results['KID_std'] = kid_std.item()
    except Exception as e:
        print(f"KID calculation failed: {e}")
        metrics_results['KID_mean'] = None
        metrics_results['KID_std'] = None

    # 3. LPIPS Score (Fixed)
    print("Computing LPIPS...")
    try:
        lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=False)
        # For LPIPS, we need paired comparisons with proper normalization [0, 1]
        min_samples = min(len(real_images), len(generated_images), 20)  # Limit for memory
        lpips_scores = []

        for i in range(min_samples):
            # Convert from [-1, 1] to [0, 1] range for LPIPS
            real_img = (real_images[i:i+1] + 1.0) / 2.0
            gen_img = (generated_images[i:i+1] + 1.0) / 2.0

            # Clamp to ensure [0, 1] range and check for NaN
            real_img = torch.clamp(real_img, 0.0, 1.0)
            gen_img = torch.clamp(gen_img, 0.0, 1.0)

            # Skip if NaN values are present
            if torch.isnan(real_img).any() or torch.isnan(gen_img).any():
                continue

            lpips_score = lpips_metric(real_img, gen_img)
            if not torch.isnan(lpips_score):
                lpips_scores.append(lpips_score.item())

        if lpips_scores:
            metrics_results['LPIPS_mean'] = np.mean(lpips_scores)
            metrics_results['LPIPS_std'] = np.std(lpips_scores)
        else:
            metrics_results['LPIPS_mean'] = None
            metrics_results['LPIPS_std'] = None

    except Exception as e:
        print(f"LPIPS calculation failed: {e}")
        metrics_results['LPIPS_mean'] = None
        metrics_results['LPIPS_std'] = None

    # 4. FLOPs Analysis (Fixed)
    print("Computing FLOPs...")
    try:
        # Create a dummy input for FLOPs calculation
        dummy_input = torch.randn(1, 3, FINE_TUNE_CONFIG["img_size"], FINE_TUNE_CONFIG["img_size"])
        dummy_t = torch.randint(0, model.denoising_steps, (1,)).long()

        # Move model to CPU temporarily for FLOPs calculation
        model_device = next(model.parameters()).device
        model.cuda()
        model.eval()

        # Count FLOPs for a single forward pass
        with torch.no_grad():
            if FlopCountAnalysis is not None:
                # Use fvcore if available
                flops_analyzer = FlopCountAnalysis(model, (dummy_input, dummy_t))
                total_flops = flops_analyzer.total()
            elif profile_macs is not None:
                # Use torchprofile as alternative
                def model_wrapper(x):
                    return model(x, dummy_t)
                total_flops = profile_macs(model_wrapper, dummy_input)
            else:
                # Estimate based on parameters
                total_params = sum(p.numel() for p in model.parameters())
                total_flops = total_params * 2  # Rough estimate

            metrics_results['FLOPs'] = int(total_flops)
            metrics_results['FLOPs_G'] = total_flops / 1e9  # GFLOPs

        # Move model back to original device
        model.to(model_device)

    except Exception as e:
        print(f"FLOPs calculation failed: {e}")
        # Estimate FLOPs based on model parameters (rough approximation)
        try:
            total_params = sum(p.numel() for p in model.parameters())
            estimated_flops = total_params * 2  # Rough estimate: 2 FLOPs per parameter
            metrics_results['FLOPs'] = estimated_flops
            metrics_results['FLOPs_G'] = estimated_flops / 1e9
            print(f"Using estimated FLOPs based on parameters: {estimated_flops/1e9:.2f}G")
        except:
            metrics_results['FLOPs'] = None
            metrics_results['FLOPs_G'] = None

    return metrics_results

# Generate samples for evaluation
print("🎯 Starting comprehensive evaluation...")

batch_size_eval = 50  # Reduced batch size to avoid memory issues

# Get real features from the validation dataset (fixed variable name)
real_features = get_real_features(val_data, batch_size_eval)

# Generate samples using improved process
print("Generating samples for evaluation...")
with torch.no_grad():
    model.eval()
    generated_samples = []

    for i in tqdm(range(batch_size_eval), desc="Generating evaluation samples"):
        # Generate random noise
        noise = torch.randn(1, 3, FINE_TUNE_CONFIG["img_size"], FINE_TUNE_CONFIG["img_size"], device=model.device)

        # Use multiple timesteps for better sampling (simplified diffusion process)
        timesteps = [50, 25, 10, 5, 1]  # Simplified sampling schedule
        x = noise

        for t_val in timesteps:
            t = torch.tensor([t_val], device=model.device).long()
            pred_noise = model(x, t)

            # Simple denoising step (simplified)
            alpha = 0.1  # Small step size
            x = x - alpha * pred_noise

        # Ensure output is in proper range [-1, 1]
        sample = torch.clamp(x, -1.0, 1.0)
        generated_samples.append(sample.cpu())

    generated_features = torch.cat(generated_samples, dim=0)

# Calculate comprehensive metrics
evaluation_results = calculate_comprehensive_metrics(
    real_features, generated_features, model
)

print("\n" + "="*60)
print("COMPREHENSIVE EVALUATION RESULTS")
print("="*60)

if evaluation_results['FID'] is not None:
    print(f"FID Score: {evaluation_results['FID']:.4f}")
    print("  → Lower is better (measures quality and diversity)")

if evaluation_results['KID_mean'] is not None:
    print(f"KID Score: {evaluation_results['KID_mean']:.6f} ± {evaluation_results['KID_std']:.6f}")
    print("  → Lower is better (unbiased estimator, more robust than FID)")

if evaluation_results['LPIPS_mean'] is not None:
    print(f"LPIPS Score: {evaluation_results['LPIPS_mean']:.4f} ± {evaluation_results['LPIPS_std']:.4f}")
    print("  → Lower is better (perceptual similarity to real images)")

if evaluation_results['FLOPs_G'] is not None:
    print(f"Computational Cost (GFlops): {evaluation_results['FLOPs_G']:.4f} GFLOPs per image")
    print(f"Computational Cost (Flops): {evaluation_results['FLOPs']} FLOPs per image")
    print("  → Model complexity for single forward pass")


print("="*60)

# Additional LoRA-specific metrics
print("\n📊 LoRA-Specific Metrics:")
print("="*40)
lora_stats = calculate_lora_parameters(model)
print(f"Total Parameters: {lora_stats['total_parameters']:,}")
print(f"LoRA Parameters: {lora_stats['lora_parameters']:,}")
print(f"Parameter Efficiency: {lora_stats['lora_ratio']:.2f}%")
print(f"Memory Savings: {100 - lora_stats['lora_ratio']:.1f}%")

"""## 9. LoRA Management and Model Export

Utilities for saving, loading, and fusing LoRA weights for deployment.
"""

# Demonstrate LoRA weight management
print("💾 LoRA Weight Management Demo")
print("=" * 40)

# Helper function for LoRA parameter summary
def get_lora_parameter_summary(model):
    """Get comprehensive LoRA parameter statistics"""
    total_lora_params = 0
    total_original_params = 0
    num_lora_modules = 0

    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            num_lora_modules += 1
            total_lora_params += module.lora_A.numel() + module.lora_B.numel()
            total_original_params += module.base.weight.numel()

    return {
        "total_lora_parameters": total_lora_params,
        "total_original_parameters": total_original_params,
        "parameter_efficiency": total_lora_params / total_original_params * 100,
        "lora_modules": num_lora_modules
    }

# Get current LoRA statistics
summary = get_lora_parameter_summary(model)
print(f"💾 LoRA weights saved to ./weights/lora_finetuned/demo_lora_weights.pth")

# Save LoRA weights
os.makedirs("./weights/lora_finetuned", exist_ok=True)
save_lora_weights(model, "./weights/lora_finetuned/demo_lora_weights.pth")
print(f"✅ LoRA weights saved to: ./weights/lora_finetuned/demo_lora_weights.pth")

print(f"\n📊 LoRA Parameter Summary:")
print(f"  • Total LoRA parameters: {summary['total_lora_parameters']:,}")
print(f"  • Total original parameters: {summary['total_original_parameters']:,}")
print(f"  • Parameter efficiency: {summary['parameter_efficiency']:.2f}%")
print(f"  • LoRA modules: {summary['lora_modules']}")

print(f"\n🔄 Creating model copy for fusion demo...")
# Create a copy of the model for fusion demonstration
import copy
model_copy = copy.deepcopy(model)
print("✅ Block-wise LoRA injection complete!")

# Show which layers were replaced
lora_layers = []
for name, module in model_copy.named_modules():
    if isinstance(module, LoRALinear):
        lora_layers.append(f"   • {name} -> rank {module.r}")

print(f"📊 Replaced {len(lora_layers)} linear layers:")
for layer_info in lora_layers:
    print(layer_info)

# Load LoRA weights into the copy
load_lora_weights(model_copy, "./weights/lora_finetuned/demo_lora_weights.pth")
print("✅ LoRA weights loaded to model copy")

print(f"\n🔍 Current Generation Method:")
print("=" * 40)
print("🎯 CURRENT: Using LoRA Addition (Not Fused)")
print("   • Forward pass: output = W₀x + (α/r)(B·A·x)")
print("   • Base weights: FROZEN (not modified)")
print("   • LoRA weights: ADDED during computation")
print("   • Advantage: Can switch between LoRA adapters")
print("   • Cost: Slight computational overhead")

print(f"\n🔗 Alternative: Fused Model Option")
print("=" * 40)
print("🎯 FUSED: Single weight matrix (optional)")
print("   • Forward pass: output = W_new·x")
print("   • Where: W_new = W₀ + (α/r)(A·B)")
print("   • Advantage: Faster inference")
print("   • Cost: Cannot switch adapters easily")

# Demonstrate the difference with a small test
print(f"\n🧪 Testing Both Approaches:")
print("=" * 40)

# Test input
test_input = torch.randn(1, 3, FINE_TUNE_CONFIG["img_size"], FINE_TUNE_CONFIG["img_size"], device=model.device)
test_t = torch.randint(0, model.denoising_steps, (1,), device=model.device).long()

# Method 1: LoRA Addition (current approach)
model.eval()
with torch.no_grad():
    output_lora_addition = model(test_input, test_t)

print(f"✅ Method 1 (LoRA Addition): Generated {output_lora_addition.shape} tensor")
print(f"   • Uses: W₀x + ΔWx computation")
print(f"   • Output range: [{output_lora_addition.min():.3f}, {output_lora_addition.max():.3f}]")

# Method 2: Fused weights (demonstration)
print(f"\n🔗 Fusing LoRA weights for deployment...")
fuse_all_lora(model_copy)
print("✅ LoRA weights fused into base model")

model_copy.eval()
with torch.no_grad():
    output_fused = model_copy(test_input, test_t)

print(f"✅ Method 2 (Fused Model): Generated {output_fused.shape} tensor")
print(f"   • Uses: W_new·x computation (W_new = W₀ + ΔW)")
print(f"   • Output range: [{output_fused.min():.3f}, {output_fused.max():.3f}]")

# Check if outputs are equivalent
difference = torch.abs(output_lora_addition - output_fused).max().item()
print(f"\n🔍 Numerical Difference: {difference:.8f}")
if difference < 1e-5:
    print("✅ Both methods produce equivalent results!")
else:
    print("⚠️ Methods produce different results (unexpected)")

print(f"\n📝 Summary for Test Image Generation:")
print("=" * 50)
print("Current notebook generates test images using:")
print("🎯 LoRA Addition method (W₀ + ΔW computed separately)")
print("📊 Base DiffiT weights remain frozen and unchanged")
print("🔧 Only LoRA parameters (A, B matrices) were trained")
print("💡 This allows flexible adapter switching and management")

"""## 10. Visualization and Analysis (Fixed)

Comprehensive visualization of training results, model performance, and LoRA efficiency.

**Fixed Issues:**
- ✅ Corrected key names in `get_lora_parameter_summary` function calls
- ✅ Fixed parameter efficiency calculation
- ✅ Updated visualization code to use correct dictionary keys
"""

import matplotlib.pyplot as plt
import seaborn as sns
from torchvision.utils import make_grid

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")

def visualize_training_progress(trainer):
    """
    Visualize training and validation loss curves
    """
    if hasattr(trainer, 'logger') and hasattr(trainer.logger, 'experiment'):
        # Get metrics from TensorBoard logs if available
        print("📈 Training progress visualization...")
        print("💡 Check TensorBoard logs for detailed training curves:")
        print(f"   tensorboard --logdir {FINE_TUNE_CONFIG['output_dir']}/logs")
    else:
        print("📊 Training metrics not available for visualization")

def get_lora_parameter_summary_fixed(model):
    """Get comprehensive LoRA parameter statistics (FIXED VERSION)"""
    total_lora_params = 0
    total_original_params = 0
    num_lora_modules = 0

    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            num_lora_modules += 1
            total_lora_params += module.lora_A.numel() + module.lora_B.numel()
            total_original_params += module.base.weight.numel()

    # Calculate efficiency ratio
    efficiency_ratio = total_lora_params / total_original_params if total_original_params > 0 else 0

    return {
        "total_lora_parameters": total_lora_params,
        "total_original_parameters": total_original_params,
        "parameter_efficiency": efficiency_ratio * 100,  # As percentage
        "efficiency_ratio": efficiency_ratio,  # As decimal
        "lora_modules": num_lora_modules,
        # Legacy keys for backward compatibility
        "total_lora_params": total_lora_params,
        "total_original_params": total_original_params
    }

def generate_sample_grid(model, num_samples=16, img_size=32):
    """
    Generate a grid of sample images
    """
    model.eval()

    print(f"🎨 Generating {num_samples} sample images...")

    with torch.no_grad():
        # Generate random noise
        noise = torch.randn(num_samples, 3, img_size, img_size, device=model.device)

        # Generate timesteps
        t = torch.randint(0, model.denoising_steps//4, (num_samples,), device=model.device).long()

        # Generate samples
        samples = model(noise, t)

        # Normalize for visualization
        samples = (samples + 1) / 2  # [-1, 1] -> [0, 1]
        samples = torch.clamp(samples, 0, 1)

        # Create grid
        grid = make_grid(samples, nrow=4, padding=2, normalize=False)
        grid_np = grid.cpu().numpy().transpose(1, 2, 0)

        return grid_np, samples

# Plot comprehensive results (FIXED VERSION)
plt.figure(figsize=(15, 10))

# Metrics visualization - similar to base DiffiT notebook layout
if 'evaluation_results' in locals():
    # FID Score
    plt.subplot(2, 3, 1)
    if evaluation_results['FID'] is not None:
        plt.bar(['FID'], [evaluation_results['FID']], color='skyblue')
        plt.title("FID Score")
        plt.ylabel("Score")
        plt.text(0, evaluation_results['FID']/2, f"{evaluation_results['FID']:.2f}",
                ha='center', va='center', fontweight='bold')

    # KID Score
    plt.subplot(2, 3, 2)
    if evaluation_results['KID_mean'] is not None:
        plt.bar(['KID'], [evaluation_results['KID_mean']], color='lightcoral')
        plt.title("KID Score")
        plt.ylabel("Score")
        plt.text(0, evaluation_results['KID_mean']/2, f"{evaluation_results['KID_mean']:.4f}",
                ha='center', va='center', fontweight='bold')

    # LPIPS Score
    plt.subplot(2, 3, 3)
    if evaluation_results['LPIPS_mean'] is not None:
        plt.bar(['LPIPS'], [evaluation_results['LPIPS_mean']], color='lightgreen')
        plt.title("LPIPS Score")
        plt.ylabel("Score")
        plt.text(0, evaluation_results['LPIPS_mean']/2, f"{evaluation_results['LPIPS_mean']:.3f}",
                ha='center', va='center', fontweight='bold')

    # FLOPs
    plt.subplot(2, 3, 4)
    if evaluation_results['FLOPs_G'] is not None:
        plt.bar(['GFLOPs'], [evaluation_results['FLOPs_G']], color='gold')
        plt.title("Computational Cost")
        plt.ylabel("GFLOPs")
        plt.text(0, evaluation_results['FLOPs_G']/2, f"{evaluation_results['FLOPs_G']:.1f}",
                ha='center', va='center', fontweight='bold')

    # LoRA Efficiency visualization (FIXED)
    plt.subplot(2, 3, 5)
    lora_summary = get_lora_parameter_summary_fixed(model)
    efficiency = lora_summary['parameter_efficiency']  # Already as percentage
    remaining = 100 - efficiency

    plt.pie([efficiency, remaining],
            labels=[f'LoRA\\n({efficiency:.1f}%)', f'Fixed\\n({remaining:.1f}%)'],
            colors=['orange', 'lightgray'],
            autopct='%1.1f%%',
            startangle=90)
    plt.title("Parameter Efficiency")

    # Parameter count comparison (FIXED)
    plt.subplot(2, 3, 6)
    categories = ['Original', 'LoRA']
    values = [lora_summary['total_original_parameters'], lora_summary['total_lora_parameters']]
    colors = ['lightblue', 'orange']

    bars = plt.bar(categories, values, color=colors, alpha=0.8)
    plt.title('Parameter Count')
    plt.ylabel('Parameters')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)

    # Add value labels on bars
    for bar, value in zip(bars, values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:,}', ha='center', va='bottom')

else:
    print("⚠️ No evaluation results available for visualization")
    print("💡 Run the evaluation cell first to generate metrics")

plt.tight_layout()
plt.show()

# Generate sample images
print("\n🎨 Generating sample visualizations...")
sample_grid, sample_tensors = generate_sample_grid(model, num_samples=16, img_size=FINE_TUNE_CONFIG["img_size"])

plt.figure(figsize=(10, 10))
plt.imshow(sample_grid)
plt.title("Generated Samples (LoRA Fine-tuned DiffiT)")
plt.axis('off')
plt.show()

# Print detailed LoRA statistics (FIXED)
print("\n📊 Detailed LoRA Statistics:")
print("=" * 50)
lora_summary = get_lora_parameter_summary_fixed(model)
print(f"Total Original Parameters: {lora_summary['total_original_parameters']:,}")
print(f"Total LoRA Parameters: {lora_summary['total_lora_parameters']:,}")
print(f"Parameter Efficiency: {lora_summary['parameter_efficiency']:.2f}%")
print(f"Memory Reduction: {100 - lora_summary['parameter_efficiency']:.1f}%")
print(f"LoRA Modules: {lora_summary['lora_modules']}")
print(f"Compression Ratio: {lora_summary['total_original_parameters'] / lora_summary['total_lora_parameters']:.1f}x")

"""## 11. Conclusion and Next Steps

Summary of the blockwise LoRA fine-tuning workflow and recommendations for further improvements.

### 🎯 What We Accomplished

This notebook demonstrated a complete **blockwise LoRA fine-tuning workflow** for DiffiT models with the following key achievements:

#### ✅ **Efficient Parameter Adaptation**
- Implemented blockwise LoRA injection targeting specific model components
- Achieved significant parameter efficiency (typically 1-5% of original parameters)
- Maintained model performance while dramatically reducing training costs

#### ✅ **Comprehensive Evaluation**
- **FID (Fréchet Inception Distance)**: Measures generation quality
- **KID (Kernel Inception Distance)**: Alternative to FID with better statistical properties
- **LPIPS (Learned Perceptual Image Patch Similarity)**: Perceptual similarity metric
- **FLOPs**: Computational complexity analysis

#### ✅ **Production-Ready Workflow**
- Modular LoRA utilities for easy weight management
- Save/load/fuse operations for deployment
- PyTorch Lightning integration for scalable training
- Comprehensive logging and monitoring

#### ✅ **Best Practices Implementation**
- Block-specific rank configuration for optimal adaptation
- Proper parameter freezing to preserve pretrained knowledge
- Gradient clipping and learning rate scheduling
- Early stopping and model checkpointing

### 🚀 **Next Steps and Improvements**

#### **Model Enhancements**
```python
# 1. Advanced LoRA Configurations
# - Adaptive rank selection based on layer importance
# - Dynamic LoRA insertion during training
# - Cross-attention specific adaptations

# 2. Multi-scale LoRA
# - Different ranks for different resolution stages
# - Progressive rank reduction for efficiency

# 3. Task-specific Adaptations  
# - Text-to-image specific LoRA patterns
# - Style transfer optimized configurations
```

#### **Training Optimizations**
```python
# 1. Advanced Schedulers
# - Cosine annealing with restarts
# - Learning rate finder integration
# - Adaptive batch size scaling

# 2. Data Augmentation
# - Advanced diffusion-aware augmentations
# - Multi-scale training strategies
# - Negative sampling techniques

# 3. Distributed Training
# - Multi-GPU LoRA synchronization
# - Gradient accumulation optimizations
# - Memory-efficient training strategies
```

#### **Evaluation Extensions**
```python
# 1. Additional Metrics
# - CLIP Score for text-image alignment
# - Inception Score (IS) for generation quality
# - Human evaluation protocols

# 2. Ablation Studies
# - Rank sensitivity analysis
# - Block-wise contribution analysis
# - Comparison with full fine-tuning

# 3. Robustness Testing
# - Out-of-distribution generalization
# - Adversarial robustness
# - Failure case analysis
```

### 📚 **Key Takeaways**

1. **Parameter Efficiency**: LoRA enables fine-tuning with <5% of original parameters
2. **Modular Design**: Block-wise injection allows targeted adaptations
3. **Scalability**: PyTorch Lightning provides enterprise-ready training infrastructure
4. **Comprehensive Evaluation**: Multiple metrics ensure robust model assessment
5. **Production Ready**: Complete save/load/fuse workflow for deployment

### 🔗 **Useful Resources**

- **LoRA Paper**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **DiffiT Paper**: [DiffiT: Diffusion Vision Transformers for Image Generation](https://arxiv.org/abs/2312.02139)
- **PyTorch Lightning**: [Official Documentation](https://pytorch-lightning.readthedocs.io/)
- **TorchMetrics**: [Image Quality Metrics](https://torchmetrics.readthedocs.io/en/stable/image/)

---

**🎉 Congratulations!** You now have a complete, production-ready workflow for efficient DiffiT fine-tuning using blockwise LoRA adaptation.
"""