# -*- coding: utf-8 -*-
"""diffit_image_space_architecture (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wmEOFgbamLaejN5h1PbcgEUXPT9JGt3X

# DiffiT: Diffusion Vision Transformers for Image Generation

This notebook implements **DiffiT (Diffusion Vision Transformers)**, a novel approach that combines the power of diffusion models with vision transformers for high-quality image generation. The implementation includes both image-space and latent-space architectures.

## Key Features:
- **Time-aware Multi-head Self-Attention (TMSA)** mechanism
- **U-shaped architecture** for image-space diffusion
- **Latent-space architecture** with patch embeddings
- Support for **CIFAR-10** and **Imagenette** datasets
- Comprehensive evaluation with **FID scores**
- **PyTorch Lightning** framework integration

## Architecture Overview:
DiffiT replaces traditional CNN-based diffusion models with transformer architectures, enabling better long-range dependencies and more efficient parameter usage for image generation tasks.

## 1. Environment Setup and Imports

First, let's install the required packages and import all necessary libraries.
"""

# Core imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import math
import numpy as np
import matplotlib.pyplot as plt
import random
import requests

# Deep learning utilities
from torchmetrics.image.fid import FrechetInceptionDistance
from scipy.stats import wasserstein_distance
from sklearn.metrics import precision_score, recall_score
from tqdm.auto import tqdm
from pytorch_lightning.callbacks import ModelCheckpoint
from torch_ema import ExponentialMovingAverage
from torch.utils.data import Dataset, DataLoader, random_split

# Dataset and image processing
from datasets import load_dataset
from torchvision import transforms
from torchvision.transforms import (
    Compose,
    Lambda,
    ToPILImage,
    Resize,
    CenterCrop,
    ToTensor,
)
from PIL import Image

print("All packages imported successfully!")

# Core imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import math
import numpy as np
import matplotlib.pyplot as plt
import random
import requests

# Deep learning utilities
from torchmetrics.image.fid import FrechetInceptionDistance
from scipy.stats import wasserstein_distance
from sklearn.metrics import precision_score, recall_score
from tqdm.auto import tqdm
from pytorch_lightning.callbacks import ModelCheckpoint
from torch_ema import ExponentialMovingAverage
from torch.utils.data import Dataset, DataLoader, random_split

# Dataset and image processing
from datasets import load_dataset
from torchvision import transforms
from torchvision.transforms import (
    Compose,
    Lambda,
    ToPILImage,
    Resize,
    CenterCrop,
    ToTensor,
)
from PIL import Image

print("All packages imported successfully!")

"""## 2. Configuration and Global Variables

Set up device configuration, model selection, and hyperparameters.
"""

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Model architecture selection
MODEL = "image-space"  # Options: "image-space" or "latent-space"

# Execution modes
IS_TRAIN = True
IS_TEST = True
IS_METRIC = True

# Global variables for tracking losses
train_losses = []
test_losses = []

# Hyperparameters based on model selection
if MODEL == "latent-space":
    DATASET_NAME = "IMAGENETTE"
    DIR_WEIGHTS = "./weights/LatentWeights/"
    PATH_WEIGHTS = "./weights/LatentWeights/best_model.ckpt"

    BATCH_SIZE_TRAIN = 32
    BATCH_SIZE_TEST = 16
    LEARNING_RATE = 0.001
    IMG_SIZE = 32
    NUM_EPOCHS = 10

    D_MODEL = 128  # TOKEN_DIM = TIME_EMBEDDING_DIM
    N_LATENT_BLOCKS = 1
    NUM_HEADS = 2
    DROPOUT_PROB = 0.1
    D_FF = 2 * D_MODEL

    T = 500  # Number of (de)noising steps

elif MODEL == "image-space":
    DATASET_NAME = "CIFAR"
    DIR_WEIGHTS = "./weights/ImageSpaceWeights/"
    PATH_WEIGHTS = "/content/best_model.ckpt"

    BATCH_SIZE_TRAIN = 64
    BATCH_SIZE_TEST = 16
    LEARNING_RATE = 0.001
    IMG_SIZE = 32
    NUM_EPOCHS = 3

    D_MODEL = 128
    NUM_HEADS = 2
    DROPOUT_PROB = 0.1
    D_FF = 2 * D_MODEL

    T = 500  # Number of (de)noising steps

# Assertions for configuration validation
assert MODEL in ["image-space", "latent-space"], "[ERROR] MODEL should be either 'image-space' or 'latent-space'."
assert IS_TRAIN or IS_TEST or IS_METRIC, "[ERROR] At least one execution mode should be enabled."

print(f"Configuration loaded for {MODEL} model")
print(f"Dataset: {DATASET_NAME}, Batch size: {BATCH_SIZE_TRAIN}, Learning rate: {LEARNING_RATE}")
print(f"Image size: {IMG_SIZE}, Model dimension: {D_MODEL}, Denoising steps: {T}")

"""## 3. Utility Functions for Diffusion Process

Core utilities for the diffusion process including noise scheduling and sampling functions.
"""

# Causal mask for attention mechanism
def causal_mask(size):
    """Create a causal mask for attention mechanism"""
    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int).to(device)
    return mask == 0

# Helper function for extracting values at specific timesteps
def extract(a, t, x_shape):
    """Extract values from tensor a at timesteps t"""
    batch_size = t.shape[0]
    out = a.gather(-1, t.cpu())
    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)

# Beta scheduling functions
def cosine_beta_schedule(timesteps, s=0.008):
    """
    Cosine schedule as proposed in https://arxiv.org/abs/2102.09672
    """
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0.0001, 0.9999)

def linear_beta_schedule(timesteps):
    """Linear beta schedule for diffusion process"""
    beta_start = 0.0001
    beta_end = 0.02
    return torch.linspace(beta_start, beta_end, timesteps)

# Forward diffusion process (q_sample)
def q_sample(x_start, t, noise):
    """Forward diffusion process: x_t-1 -> x_t"""
    # Define beta schedule
    betas = linear_beta_schedule(timesteps=T)

    # Define alphas
    alphas = 1.0 - betas
    alphas_cumprod = torch.cumprod(alphas, axis=0)

    # Calculations for diffusion q(x_t | x_{t-1}) and others
    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)

    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)
    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)

    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise

# Loss computation
def p_losses(denoise_model, x_start, t, l=None):
    """Compute diffusion loss"""
    # Get Gaussian noise
    noise = torch.randn_like(x_start)

    # Apply the noise to the image
    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)
    predicted_noise = denoise_model(x_noisy, t, l)

    # Use smooth L1 loss instead of MSE
    return F.smooth_l1_loss(noise, predicted_noise)

# Reverse diffusion process (p_sample)
@torch.no_grad()
def p_sample(model, x, t, t_index):
    """Single step of reverse diffusion process"""
    # Define beta schedule
    betas = linear_beta_schedule(timesteps=T)

    # Define alphas
    alphas = 1.0 - betas
    alphas_cumprod = torch.cumprod(alphas, axis=0)
    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)

    # Calculations for diffusion q(x_t | x_{t-1}) and others
    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)

    # Calculations for posterior q(x_{t-1} | x_t, x_0)
    posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)

    betas_t = extract(betas, t, x.shape)
    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)
    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)

    # Equation 11 in the paper
    # Use our model (noise predictor) to predict the mean
    model_mean = sqrt_recip_alphas_t * (
        x - betas_t * model(x, t, None) / sqrt_one_minus_alphas_cumprod_t
    )

    if t_index == 0:
        return model_mean
    else:
        posterior_variance_t = extract(posterior_variance, t, x.shape)
        noise = torch.randn_like(x)
        # Algorithm 2 line 4:
        return model_mean + torch.sqrt(posterior_variance_t) * noise

# Complete sampling loop
@torch.no_grad()
def p_sample_loop(model, shape):
    """Complete reverse diffusion sampling loop"""
    device = next(model.parameters()).device

    b = shape[0]
    # Start from pure noise (for each example in the batch)
    img = torch.randn(shape, device=device)
    imgs = []

    for i in tqdm(reversed(range(0, T)), desc="sampling loop time step", total=T):
        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)
        imgs.append(img.cpu().numpy())
    return imgs

@torch.no_grad()
def sample(model, image_size, batch_size=16, channels=3):
    """Generate samples from the model"""
    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))

"""## 4. Core Model Components

Fundamental building blocks for the DiffiT architecture including layer normalization, MLP, and attention mechanisms.
"""

class LayerNormalization(nn.Module):
    """Custom layer normalization implementation"""
    def __init__(self, eps: float = 10**-6):
        super().__init__()
        self.eps = eps
        self.alpha = nn.Parameter(torch.ones(1))  # Multiplies
        self.bias = nn.Parameter(torch.zeros(1))  # Added

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        return self.alpha * (x - mean) / (std + self.eps) + self.bias

class MLP(nn.Module):
    """Multi-layer perceptron with GELU activation"""
    def __init__(self, img_size: int, d_ff: int):
        super().__init__()
        self.linear_1 = nn.Linear(img_size, d_ff)
        self.gelu = nn.GELU()
        self.linear_2 = nn.Linear(d_ff, img_size)

    def forward(self, x):
        out_linear_1 = self.linear_1(x)
        out_gelu = self.gelu(out_linear_1)
        out_linear_2 = self.linear_2(out_gelu)
        return out_linear_2

class SinusoidalPositionEmbeddings(nn.Module):
    """Sinusoidal position embeddings for time steps"""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2

        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings

class TimeEmbedding(nn.Module):
    """Time embedding module for diffusion timesteps"""
    def __init__(self, d_model: int, seq_len: int):
        super().__init__()
        self.seq_len = seq_len
        self.d_model = d_model

        self.time_embedding_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(seq_len),
            nn.Linear(seq_len, d_model),
            nn.SiLU(),
            nn.Linear(d_model, d_model),
        )

    def forward(self, time_steps):
        return self.time_embedding_mlp(time_steps)

class LabelEmbedding(nn.Module):
    """Label embedding for conditional generation"""
    def __init__(self, label_size: int, d_model: int):
        super().__init__()
        self.label_size = label_size
        self.embedding_layer = nn.Embedding(label_size, d_model)
        self.linear_layer = nn.Linear(d_model, d_model)

    def forward(self, l):
        return self.linear_layer(self.embedding_layer(l))

class TMSA(nn.Module):
    """Time-aware Multi-head Self-Attention (TMSA) - Core innovation of DiffiT"""
    def __init__(self, d_model: int, num_heads: int, dropout: float, img_size: int):
        super().__init__()
        self.space_embedding_size = d_model
        self.time_embedding_size = d_model
        self.d_model = d_model
        self.num_heads = num_heads
        self.seq_len = img_size * img_size
        self.img_size = img_size
        self.d = d_model // num_heads
        self.mask = causal_mask(self.seq_len)

        assert d_model % num_heads == 0, "d_model is not divisible by num_heads!"

        # Linear projections for spatial features (xs)
        self.Wqs = nn.Linear(d_model, d_model, bias=False)
        self.Wks = nn.Linear(d_model, d_model, bias=False)
        self.Wvs = nn.Linear(d_model, d_model, bias=False)

        # Linear projections for temporal features (xt)
        self.Wqt = nn.Linear(d_model, d_model, bias=False)
        self.Wkt = nn.Linear(d_model, d_model, bias=False)
        self.Wvt = nn.Linear(d_model, d_model, bias=False)

        self.WK = nn.Linear(self.d, self.seq_len, bias=False)
        self.wo = nn.Linear(d_model, d_model, bias=False)
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def compute_attention_scores(query, key, value, wK, mask, dropout: nn.Dropout):
        """Compute attention scores with time-aware bias"""
        d = query.shape[-1]

        attention_scores = (query @ key.transpose(-2, -1) + wK(query)) / math.sqrt(d)

        # Apply mask if required
        if mask is not None:
            attention_scores.masked_fill_(mask == 0, -1e9)

        # Apply softmax
        attention_scores = F.softmax(attention_scores, dim=-1)

        # Apply dropout if required
        if dropout is not None:
            attention_scores = dropout(attention_scores)

        return attention_scores @ value

    def forward(self, xs, xt):
        xs = xs.view(xs.shape[0], self.seq_len, xs.shape[1])

        # Space query, key and value
        query_s = self.Wqs(xs)
        key_s = self.Wks(xs)
        value_s = self.Wvs(xs)

        qs_1 = query_s.view(query_s.shape[0], query_s.shape[1], self.num_heads, self.d).transpose(1, 2)
        ks_1 = key_s.view(key_s.shape[0], key_s.shape[1], self.num_heads, self.d).transpose(1, 2)
        vs_1 = value_s.view(value_s.shape[0], value_s.shape[1], self.num_heads, self.d).transpose(1, 2)

        # Temporal query, key and value
        query_t = self.Wqt(xt)
        key_t = self.Wkt(xt)
        value_t = self.Wvt(xt)

        qt_1 = query_t.view(query_t.shape[0], -1, self.num_heads, self.d).transpose(1, 2)
        kt_1 = key_t.view(key_t.shape[0], -1, self.num_heads, self.d).transpose(1, 2)
        vt_1 = value_t.view(value_t.shape[0], -1, self.num_heads, self.d).transpose(1, 2)

        # Concatenation of spatial and temporal features
        qs = qs_1 + qt_1
        ks = ks_1 + kt_1
        vs = vs_1 + vt_1

        # Compute attention scores
        h = self.compute_attention_scores(qs, ks, vs, self.WK, self.mask, self.dropout)

        # Combine all the heads together
        h = h.transpose(1, 2).contiguous().view(h.shape[0], -1, self.num_heads * self.d)

        output = self.wo(h)
        output = output.view(h.shape[0], h.shape[2], int(math.sqrt(h.shape[1])), -1)

        return output

"""## 5. DiffiT Architecture Classes

Main architectural components including DiffiT blocks, encoder-decoder structure, and complete networks.
"""

class DiffiTBlock(nn.Module):
    """Core DiffiT transformer block"""
    def __init__(self, d_model: int, num_heads: int, dropout: float, d_ff: int,
                 img_size: int, label_size: int = None):
        super().__init__()
        self.ln = LayerNormalization()
        self.tmsa = TMSA(d_model, num_heads, dropout, img_size)
        self.mlp = MLP(img_size, d_ff)
        self.time_embedding = TimeEmbedding(d_model, img_size * img_size)

        # Only for latent model
        if label_size is not None:
            self.label_size = label_size
            self.label_embedding = LabelEmbedding(label_size, d_model)

    def forward(self, xs, t, l=None):
        xt = self.time_embedding(t)
        tmsa_comb = xt

        if l is not None:
            tmsa_comb += self.label_embedding(l)

        xs1 = self.tmsa(self.ln(xs), tmsa_comb) + xs
        xs2 = self.mlp(self.ln(xs1)) + xs1

        return xs2

class Tokenizer(nn.Module):
    """Convert image patches to tokens"""
    def __init__(self, out_channels: int, in_channels=3):
        super().__init__()
        self.conv3x3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                                kernel_size=3, padding=1)

    def forward(self, x):
        return self.conv3x3(x)

class Head(nn.Module):
    """Output head to convert tokens back to image"""
    def __init__(self, in_channels: int, out_channels=3):
        super().__init__()
        self.group_norm = nn.GroupNorm(num_groups=in_channels // 4, num_channels=in_channels)
        self.conv3x3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                                kernel_size=3, padding=1)

    def forward(self, x):
        return self.conv3x3(self.group_norm(x))

class DiffiTResBlock(nn.Module):
    """DiffiT residual block with convolution and transformer"""
    def __init__(self, in_channels: int, out_channels: int, d_model: int, num_heads: int,
                 dropout: float, d_ff: int, img_size: int, device, label_size: int = None):
        super().__init__()
        self.device = device
        self.seq_len = img_size * img_size

        self.conv3x3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                                kernel_size=3, padding=1)
        self.swish = nn.SiLU()
        self.group_norm = nn.GroupNorm(num_groups=in_channels // 4, num_channels=in_channels)
        self.diffit_block = DiffiTBlock(out_channels, num_heads, dropout, d_ff, img_size, label_size)

    def forward(self, xs, t, l=None):
        xs_1 = self.conv3x3(self.swish(self.group_norm(xs)))
        xs = xs + self.diffit_block(xs_1, t, l)
        return xs

class Downsample(nn.Module):
    """Downsampling layer"""
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3,
                 stride: int = 2, padding: int = 1):
        super().__init__()
        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                             kernel_size=kernel_size, stride=stride, padding=padding)

    def forward(self, x):
        return self.conv(x)

class Upsample(nn.Module):
    """Upsampling layer"""
    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3,
                 stride: int = 2, padding: int = 1, output_padding: int = 1):
        super().__init__()
        self.conv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,
                                      kernel_size=kernel_size, stride=stride,
                                      padding=padding, output_padding=output_padding)

    def forward(self, x):
        return self.conv(x)

class ResBlockGroup(nn.Module):
    """Group of residual blocks"""
    def __init__(self, d_model: int, num_heads: int, dropout: float, d_ff: int, L: int,
                 in_channels: int, out_channels: int, img_size: int, device, label_size: int = None):
        super().__init__()
        self.L = L
        self.diffit_res_block = DiffiTResBlock(in_channels, out_channels, d_model, num_heads,
                                              dropout, d_ff, img_size, device, label_size)

    def forward(self, x, t, l=None):
        for _ in range(self.L):
            x = self.diffit_res_block(x, t, l)
        return x

# Complete U-shaped network for image-space DiffiT
class UShapedNetwork(pl.LightningModule):
    """U-shaped DiffiT network for image-space diffusion"""
    def __init__(self, learning_rate: float, d_model: int, num_heads: int, dropout: float,
                 d_ff: int, img_size: int, device, denoising_steps: int,
                 L1: int = 2, L2: int = 2, L3: int = 2, L4: int = 2):
        super().__init__()
        d_model_2 = d_model * 2

        self.learning_rate = learning_rate
        self.denoising_steps = denoising_steps

        self.diffit_res_block_group_1 = ResBlockGroup(d_model, num_heads, dropout, d_ff, L1,
                                                     in_channels=d_model, out_channels=d_model,
                                                     img_size=img_size, device=device)
        self.diffit_res_block_group_2 = ResBlockGroup(d_model, num_heads, dropout, d_ff, L2,
                                                     in_channels=d_model_2, out_channels=d_model_2,
                                                     img_size=img_size // 2, device=device)
        self.diffit_res_block_group_3 = ResBlockGroup(d_model, num_heads, dropout, d_ff, L3,
                                                     in_channels=d_model_2, out_channels=d_model_2,
                                                     img_size=img_size // 4, device=device)

        self.downsample_1 = Downsample(in_channels=d_model, out_channels=d_model_2)
        self.downsample_2 = Downsample(in_channels=d_model_2, out_channels=d_model_2)

        self.upsample_1 = Upsample(in_channels=d_model_2, out_channels=d_model_2)
        self.upsample_2 = Upsample(in_channels=d_model_2, out_channels=d_model)

        self.tokenizer = Tokenizer(out_channels=d_model)
        self.head = Head(in_channels=d_model)

    def uShape(self, xs, t):
        """U-shaped forward pass"""
        output_downsample_1 = self.downsample_1(self.diffit_res_block_group_1(xs, t))
        output_downsample_2 = self.downsample_2(self.diffit_res_block_group_2(output_downsample_1, t))
        uLeft = output_downsample_2

        uCenter = self.diffit_res_block_group_3(uLeft, t)

        input_upsample_1 = uCenter + uLeft
        input_upsample_2 = (self.diffit_res_block_group_2(self.upsample_1(input_upsample_1), t)
                           + output_downsample_1)
        uRight = self.diffit_res_block_group_1(self.upsample_2(input_upsample_2), t)

        return uRight

    def forward(self, xs, t, l=None):
        return self.head(self.uShape(self.tokenizer(xs), t))

    def training_step(self, batch, batch_idx):
        t = torch.randint(0, self.denoising_steps, (len(batch),), device=self.device).long()
        loss = p_losses(self, batch, t)

        if batch_idx % 25 == 0:
            print(f"loss: {loss:.2f}")
            train_losses.append(loss.cpu().detach().numpy())

        self.log("train_loss", loss)
        return loss

    def test_step(self, batch, batch_idx):
        t = torch.randint(0, self.denoising_steps, (len(batch),), device=self.device).long()
        loss = p_losses(self, batch, t)
        test_losses.append(loss.cpu().detach().numpy())

        self.log("test_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        return optimizer

"""## 6. Dataset Loading and Preprocessing

Load and preprocess CIFAR-10 and Imagenette datasets for training and evaluation.
"""

def get_cifar_ds():
    """Load and preprocess CIFAR-10 dataset"""
    dataset = load_dataset("cifar10")

    train = dataset["train"]
    test = dataset["test"]

    train_tensor = []
    trans = transforms.ToTensor()
    for image in train:
        image_tensor = trans(image["img"])
        train_tensor.append(image_tensor)

    test_tensor = []
    for image in test:
        image_tensor = trans(image["img"])
        test_tensor.append(image_tensor)

    return train_tensor, test_tensor

def get_imagenette_ds():
    """Load and preprocess Imagenette dataset"""
    dataset = load_dataset("frgfm/imagenette", "160px")

    train = dataset["train"]
    test = dataset["validation"]

    trans = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.ToTensor(),
    ])

    train_tensor = []
    for image in train:
        image_tensor = trans(image["image"])
        if image_tensor.shape[0] == 3:  # Only RGB images
            train_tensor.append(image_tensor)

    test_tensor = []
    for image in test:
        image_tensor = trans(image["image"])
        if image_tensor.shape[0] == 3:  # Only RGB images
            test_tensor.append(image_tensor)

    return train_tensor, test_tensor

def get_ds(ds_name: str):
    """Get dataset by name"""
    if ds_name == "CIFAR":
        return get_cifar_ds()
    elif ds_name == "IMAGENETTE":
        return get_imagenette_ds()
    else:
        return None

# Load the selected dataset
print(f"Loading {DATASET_NAME} dataset...")
train_ds, test_ds = get_ds(ds_name=DATASET_NAME)

# Create PyTorch DataLoaders
train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=2)
test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE_TEST, shuffle=False, num_workers=2)

print(f"Dataset loaded successfully!")
print(f"Training samples: {len(train_ds)}")
print(f"Test samples: {len(test_ds)}")
print(f"Training batches: {len(train_dataloader)}")
print(f"Test batches: {len(test_dataloader)}")

"""## 7. Model Training and Testing

Initialize the DiffiT model and run training/testing procedures.
"""

# Model checkpoint callback
checkpoint_callback = ModelCheckpoint(
    dirpath=DIR_WEIGHTS,
    filename="best_model",
    monitor="train_loss",
    mode="min",
    verbose=True,
)

# Initialize DiffiT model based on selected architecture
if MODEL == "image-space":
    if IS_TRAIN:
        diffit_model = UShapedNetwork(
            learning_rate=LEARNING_RATE,
            d_model=D_MODEL,
            num_heads=NUM_HEADS,
            dropout=DROPOUT_PROB,
            d_ff=D_FF,
            img_size=IMG_SIZE,
            device=device,
            denoising_steps=T,
        )
        print("Created new U-shaped DiffiT model for training")
    elif IS_TEST or IS_METRIC:
        diffit_model = UShapedNetwork.load_from_checkpoint(
            PATH_WEIGHTS,
            learning_rate=LEARNING_RATE,
            d_model=D_MODEL,
            num_heads=NUM_HEADS,
            dropout=DROPOUT_PROB,
            d_ff=D_FF,
            img_size=IMG_SIZE,
            device=device,
            denoising_steps=T,
        )
        print("Loaded pre-trained U-shaped DiffiT model")

# Move model to device
diffit_model = diffit_model.to(device)
print(f"Model moved to {device}")

# Display model information
total_params = sum(p.numel() for p in diffit_model.parameters())
trainable_params = sum(p.numel() for p in diffit_model.parameters() if p.requires_grad)
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Training phase
if IS_TRAIN:
    print("Starting training...")

    # Create trainer
    trainer = pl.Trainer(
        max_epochs=NUM_EPOCHS,
        accelerator="auto",
        callbacks=[checkpoint_callback],
        log_every_n_steps=1,
    )

    # Train the model
    trainer.fit(diffit_model, train_dataloader)
    print("Training completed!")

# Testing phase
if IS_TEST:
    print("Starting testing...")

    # Create trainer for testing
    trainer = pl.Trainer(
        max_epochs=1,
        accelerator="auto",
        callbacks=[checkpoint_callback]
    )

    # Test the model
    trainer.test(diffit_model, test_dataloader)
    print("Testing completed!")

"""## 8. Image Generation and Sampling

Demonstrate the reverse diffusion process and generate new images from noise.
"""

def show_images_grid(images_list, title="Generated Images"):
    """Display images in a grid layout"""
    images_per_row = 5
    image_size = 2

    num_rows = len(images_list) // images_per_row
    if len(images_list) % images_per_row != 0:
        num_rows += 1

    fig, axes = plt.subplots(
        num_rows,
        images_per_row,
        figsize=(images_per_row * image_size, num_rows * image_size),
    )

    # Flatten the axes array if it's a multi-row layout
    if num_rows > 1:
        axes = axes.flatten()
    elif images_per_row > 1:
        axes = axes
    else:
        axes = [axes]

    for i, img in enumerate(images_list):
        if i >= len(axes):
            break

        img_tensor = torch.tensor(img)
        ax = axes[i]
        ax.imshow(transforms.ToPILImage()(img_tensor).convert("RGB"))
        ax.axis("off")

    # Remove any empty subplots
    for i in range(len(images_list), len(axes)):
        fig.delaxes(axes[i])

    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    plt.show()

def get_noisy_image(x_start, t):
    """Get noisy version of image at timestep t"""
    noise = torch.randn_like(x_start)
    x_noisy = q_sample(x_start, t=t, noise=noise).cpu().detach().numpy()
    return x_noisy

# Demonstrate the noisification process
print("Demonstrating forward diffusion process (noisification)...")

x_start = train_ds[0]
samples_to_print = []
time_steps = list(range(0, T, 50))
time_steps.append(T - 1)

for t in time_steps:
    noisy_image = get_noisy_image(x_start, torch.tensor([t]))
    samples_to_print.append(noisy_image)

show_images_grid(samples_to_print, "Forward Diffusion Process (Noisification)")

# Generate new images using the trained model
if IS_METRIC:
    print("Generating new images...")

    channels = 3
    batch_size_generation = 16

    # Generate samples using the complete reverse diffusion process
    samples = sample(
        diffit_model.to(device),
        image_size=IMG_SIZE,
        batch_size=batch_size_generation,
        channels=channels,
    )

    print("Image generation completed!")

    # Show the reverse diffusion process
    print("Reverse diffusion process (denoising):")
    samples_to_print = []
    time_steps = list(range(0, T, 50))
    time_steps.append(T - 1)
    for t in time_steps:
        samples_to_print.append(samples[t][0])

    show_images_grid(samples_to_print, "Reverse Diffusion Process (Denoising)")

    # Show final generated images
    print("Final generated images:")
    show_images_grid(samples[-1], "Final Generated Images")

"""## 9. Comprehensive Metrics Evaluation

Evaluate the model performance using multiple metrics:
- **FID (Fréchet Inception Distance)**: Measures quality and diversity of generated images
- **KID (Kernel Inception Distance)**: Alternative to FID with better statistical properties
- **LPIPS (Learned Perceptual Image Patch Similarity)**: Measures perceptual similarity
- **FLOPs**: Computational complexity analysis
"""

# Additional imports for comprehensive evaluation
try:
    from torchmetrics.image.kid import KernelInceptionDistance
    from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
    import lpips
    # Updated imports for FLOPs calculation with better error handling
    flop_count = None
    profile_macs = None
    try:
        from fvcore.nn import flop_count
        print("Using fvcore for FLOPs calculation")
    except ImportError:
        try:
            from torchprofile import profile_macs
            print("Using torchprofile for FLOPs calculation")
        except ImportError:
            print("No FLOPs calculation library available")
    print("All available evaluation metrics imported successfully!")
except ImportError as e:
    print(f"Some evaluation packages need to be installed: {e}")
    print("Run: !pip install lpips torchprofile")

def get_real_features(dataset, batch_size):
    """Sample real images for metric computation"""
    # Ensure we don't sample more than available
    batch_size = min(batch_size, len(dataset))
    items = random.sample(dataset, batch_size)
    real_features = torch.stack(items)
    return real_features

def normalize_images_for_metrics(images):
    """Normalize images to [0, 1] range and convert to uint8"""
    # Handle different input formats
    if isinstance(images, list):
        images = torch.stack(images)

    # Normalize from [-1, 1] to [0, 1] if needed
    if images.min() < 0:
        images = (images + 1) / 2

    # Ensure [0, 1] range
    images = torch.clamp(images, 0, 1)

    # Convert to uint8 for metrics
    return (images * 255).clamp(0, 255).to(torch.uint8)

def calculate_comprehensive_metrics(real_images, generated_images, model):
    """Calculate FID, KID, LPIPS, and FLOPs metrics with robust error handling"""
    metrics_results = {}

    print("Calculating comprehensive evaluation metrics...")

    # Ensure we have enough samples
    max_samples = min(len(real_images), len(generated_images), 50)
    if max_samples < 10:
        print(f"Warning: Only {max_samples} samples available. Some metrics may be unreliable.")

    # Prepare images for metrics
    real_images_subset = real_images[:max_samples]
    gen_images_subset = generated_images[:max_samples]

    # Normalize images for FID/KID (needs uint8, 0-255)
    real_images_uint8 = normalize_images_for_metrics(real_images_subset)
    gen_images_uint8 = normalize_images_for_metrics(gen_images_subset)

    # 1. FID Score
    print("Computing FID...")
    try:
        fid = FrechetInceptionDistance(feature=64, normalize=True)
        fid.update(real_images_uint8, real=True)
        fid.update(gen_images_uint8, real=False)
        fid_score = fid.compute()
        metrics_results['FID'] = fid_score.item()
        print(f"✓ FID calculated: {fid_score.item():.4f}")
    except Exception as e:
        print(f"✗ FID calculation failed: {e}")
        metrics_results['FID'] = None

    # 2. KID Score
    print("Computing KID...")
    try:
        if max_samples < 10:
            print("✗ Not enough samples for KID calculation (need at least 10)")
            metrics_results['KID_mean'] = None
            metrics_results['KID_std'] = None
        else:
            # Ensure subset_size is smaller than available samples
            subset_size = min(max_samples - 2, 25)
            kid = KernelInceptionDistance(feature=64, subset_size=subset_size, normalize=True)
            kid.update(real_images_uint8, real=True)
            kid.update(gen_images_uint8, real=False)
            kid_mean, kid_std = kid.compute()
            metrics_results['KID_mean'] = kid_mean.item()
            metrics_results['KID_std'] = kid_std.item()
            print(f"✓ KID calculated: {kid_mean.item():.6f} ± {kid_std.item():.6f}")
    except Exception as e:
        print(f"✗ KID calculation failed: {e}")
        metrics_results['KID_mean'] = None
        metrics_results['KID_std'] = None

    # 3. LPIPS Score
    print("Computing LPIPS...")
    try:
        lpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=True)
        lpips_scores = []

        # Use original tensors for LPIPS (expects [-1, 1] range)
        for i in range(min(max_samples, 25)):  # Limit to 25 for speed
            real_img = real_images_subset[i:i+1] if isinstance(real_images_subset, torch.Tensor) else torch.tensor(real_images_subset[i:i+1])
            gen_img = gen_images_subset[i:i+1] if isinstance(gen_images_subset, torch.Tensor) else torch.tensor(gen_images_subset[i:i+1])

            # Ensure proper normalization for LPIPS (expects [-1, 1])
            if real_img.max() > 1:
                real_img = real_img / 255.0 * 2 - 1  # [0, 255] -> [-1, 1]
            if gen_img.max() > 1:
                gen_img = gen_img / 255.0 * 2 - 1   # [0, 255] -> [-1, 1]

            # Ensure exact [-1, 1] range
            real_img = torch.clamp(real_img, -1.0, 1.0)
            gen_img = torch.clamp(gen_img, -1.0, 1.0)

            lpips_score = lpips_metric(real_img, gen_img)
            lpips_scores.append(lpips_score.item())

        if lpips_scores:
            metrics_results['LPIPS_mean'] = np.mean(lpips_scores)
            metrics_results['LPIPS_std'] = np.std(lpips_scores)
            print(f"✓ LPIPS calculated: {np.mean(lpips_scores):.4f} ± {np.std(lpips_scores):.4f}")
        else:
            metrics_results['LPIPS_mean'] = None
            metrics_results['LPIPS_std'] = None
    except Exception as e:
        print(f"✗ LPIPS calculation failed: {e}")
        metrics_results['LPIPS_mean'] = None
        metrics_results['LPIPS_std'] = None

    # 4. FLOPs Analysis
    print("Computing FLOPs...")
    try:
        dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)
        dummy_t = torch.randint(0, T, (1,), device=device).long()

        model.eval()
        total_flops = None

        with torch.no_grad():
            if flop_count is not None:
                try:
                    flops_dict, _ = flop_count(
                        model,
                        (dummy_input, dummy_t),
                        supported_ops={"aten::addmm", "aten::bmm", "aten::conv2d", "aten::mul"}
                    )
                    total_flops = sum(flops_dict.values())
                    print(f"✓ FLOPs calculated using fvcore: {total_flops/1e9:.2f}G")
                except Exception as e:
                    print(f"fvcore failed: {e}")

            if total_flops is None and profile_macs is not None:
                try:
                    def model_wrapper(x):
                        return model(x, dummy_t)
                    total_flops = profile_macs(model_wrapper, dummy_input)
                    print(f"✓ FLOPs calculated using torchprofile: {total_flops/1e9:.2f}G")
                except Exception as e:
                    print(f"torchprofile failed: {e}")

            if total_flops is not None:
                metrics_results['FLOPs'] = total_flops
                metrics_results['FLOPs_G'] = total_flops / 1e9
            else:
                # Parameter-based estimation
                total_params = sum(p.numel() for p in model.parameters())
                estimated_flops = total_params * 2  # Rough estimate
                metrics_results['FLOPs'] = estimated_flops
                metrics_results['FLOPs_G'] = estimated_flops / 1e9
                print(f"✓ FLOPs estimated from parameters: {estimated_flops/1e9:.2f}G")

    except Exception as e:
        print(f"✗ FLOPs calculation completely failed: {e}")
        metrics_results['FLOPs'] = None
        metrics_results['FLOPs_G'] = None

    return metrics_results

# Calculate all metrics if in evaluation mode
if IS_METRIC:
    print("Starting comprehensive evaluation...")

    batch_size_eval = 50  # Reduced batch size to avoid memory issues

    # Get real and generated features
    real_features = get_real_features(train_ds, batch_size_eval)

    # Ensure generated_features is properly formatted
    if 'samples' in locals() and samples:
        generated_features = torch.tensor(samples[-1][:batch_size_eval])
        print(f"Using {len(generated_features)} generated samples")
    else:
        print("No generated samples found. Please run the generation step first.")
        generated_features = torch.randn(batch_size_eval, 3, IMG_SIZE, IMG_SIZE)

    # Calculate comprehensive metrics
    evaluation_results = calculate_comprehensive_metrics(
        real_features, generated_features, diffit_model
    )

    print("\n" + "="*60)
    print("COMPREHENSIVE EVALUATION RESULTS")
    print("="*60)

    if evaluation_results['FID'] is not None:
        print(f"FID Score: {evaluation_results['FID']:.4f}")
        print("  → Lower is better (measures quality and diversity)")

    if evaluation_results['KID_mean'] is not None:
        print(f"KID Score: {evaluation_results['KID_mean']:.6f} ± {evaluation_results['KID_std']:.6f}")
        print("  → Lower is better (unbiased estimator, more robust than FID)")

    if evaluation_results['LPIPS_mean'] is not None:
        print(f"LPIPS Score: {evaluation_results['LPIPS_mean']:.4f} ± {evaluation_results['LPIPS_std']:.4f}")
        print("  → Lower is better (perceptual similarity to real images)")

    if evaluation_results['FLOPs_G'] is not None:
        print(f"Computational Cost: {evaluation_results['FLOPs_G']:.2f} GFLOPs per image")
        print("  → Model complexity for single forward pass")

    print("="*60)

"""## 10. Results Visualization

Visualize training progress and compare results between different model configurations.
"""

# Plot training and testing losses
plt.figure(figsize=(15, 10))

# Loss plots
plt.subplot(2, 3, 1)
if train_losses:
    plt.plot(train_losses)
    plt.title("Training Loss")
    plt.xlabel("Batch")
    plt.ylabel("Loss")
    plt.grid(True)
else:
    plt.text(0.5, 0.5, "No training data", ha='center', va='center', transform=plt.gca().transAxes)
    plt.title("Training Loss")

plt.subplot(2, 3, 2)
if test_losses:
    plt.plot(test_losses)
    plt.title("Test Loss")
    plt.xlabel("Batch")
    plt.ylabel("Loss")
    plt.grid(True)
else:
    plt.text(0.5, 0.5, "No test data", ha='center', va='center', transform=plt.gca().transAxes)
    plt.title("Test Loss")

# Metrics visualization
if IS_METRIC and 'evaluation_results' in locals():
    # FID Score
    plt.subplot(2, 3, 3)
    if evaluation_results['FID'] is not None:
        plt.bar(['FID'], [evaluation_results['FID']], color='skyblue')
        plt.title("FID Score")
        plt.ylabel("Score")
        plt.text(0, evaluation_results['FID']/2, f"{evaluation_results['FID']:.2f}",
                ha='center', va='center', fontweight='bold')

    # KID Score
    plt.subplot(2, 3, 4)
    if evaluation_results['KID_mean'] is not None:
        plt.bar(['KID'], [evaluation_results['KID_mean']], color='lightcoral')
        plt.title("KID Score")
        plt.ylabel("Score")
        plt.text(0, evaluation_results['KID_mean']/2, f"{evaluation_results['KID_mean']:.4f}",
                ha='center', va='center', fontweight='bold')

    # LPIPS Score
    plt.subplot(2, 3, 5)
    if evaluation_results['LPIPS_mean'] is not None:
        plt.bar(['LPIPS'], [evaluation_results['LPIPS_mean']], color='lightgreen')
        plt.title("LPIPS Score")
        plt.ylabel("Score")
        plt.text(0, evaluation_results['LPIPS_mean']/2, f"{evaluation_results['LPIPS_mean']:.3f}",
                ha='center', va='center', fontweight='bold')

    # FLOPs
    plt.subplot(2, 3, 6)
    if evaluation_results['FLOPs_G'] is not None:
        plt.bar(['GFLOPs'], [evaluation_results['FLOPs_G']], color='gold')
        plt.title("Computational Cost")
        plt.ylabel("GFLOPs")
        plt.text(0, evaluation_results['FLOPs_G']/2, f"{evaluation_results['FLOPs_G']:.1f}",
                ha='center', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

# Summary of results
print("\n" + "="*60)
print("EXPERIMENT SUMMARY")
print("="*60)
print(f"Model Architecture: {MODEL}")
print(f"Dataset: {DATASET_NAME}")
print(f"Image Size: {IMG_SIZE}x{IMG_SIZE}")
print(f"Model Dimension: {D_MODEL}")
print(f"Number of Heads: {NUM_HEADS}")
print(f"Dropout Probability: {DROPOUT_PROB}")
print(f"Learning Rate: {LEARNING_RATE}")
print(f"Number of Epochs: {NUM_EPOCHS}")
print(f"Denoising Steps: {T}")

# Display comprehensive metrics
if IS_METRIC and 'evaluation_results' in locals():
    print(f"\nEVALUATION METRICS:")
    print(f"─────────────────────")
    if evaluation_results['FID'] is not None:
        print(f"FID Score: {evaluation_results['FID']:.4f}")
    if evaluation_results['KID_mean'] is not None:
        print(f"KID Score: {evaluation_results['KID_mean']:.6f} ± {evaluation_results['KID_std']:.6f}")
    if evaluation_results['LPIPS_mean'] is not None:
        print(f"LPIPS Score: {evaluation_results['LPIPS_mean']:.4f} ± {evaluation_results['LPIPS_std']:.4f}")
    if evaluation_results['FLOPs_G'] is not None:
        print(f"Computational Cost: {evaluation_results['FLOPs_G']:.2f} GFLOPs")

print("="*60)

# Interpretation guide
if IS_METRIC:
    print("\nMETRICS INTERPRETATION GUIDE:")
    print("─────────────────────────────")
    print("• FID (Fréchet Inception Distance): Lower is better")
    print("  - Measures quality and diversity of generated images")
    print("  - Good: < 10, Excellent: < 5")
    print("• KID (Kernel Inception Distance): Lower is better")
    print("  - More robust alternative to FID with better statistical properties")
    print("  - Less biased for small sample sizes")
    print("• LPIPS (Learned Perceptual Image Patch Similarity): Lower is better")
    print("  - Measures perceptual similarity using deep features")
    print("  - Values typically range from 0.0 to 1.0")
    print("• FLOPs (Floating Point Operations): Lower is more efficient")
    print("  - Measures computational complexity per image generation")
    print("  - Helps assess model efficiency and deployment feasibility")

"""## Conclusion

This notebook demonstrates the complete implementation of **DiffiT (Diffusion Vision Transformers)** for image generation with comprehensive evaluation metrics.

### Key Innovations:
1. **Time-aware Multi-head Self-Attention (TMSA)**: Novel attention mechanism that incorporates temporal information from the diffusion process
2. **Transformer-based Architecture**: Replaces traditional CNN-based diffusion models with vision transformers
3. **Dual Architecture Support**: Both image-space and latent-space implementations

### Comprehensive Evaluation:
This implementation includes state-of-the-art evaluation metrics:
- **FID (Fréchet Inception Distance)**: Industry standard for measuring image quality and diversity
- **KID (Kernel Inception Distance)**: More robust alternative to FID with better statistical properties
- **LPIPS (Learned Perceptual Image Patch Similarity)**: Measures perceptual similarity using deep neural networks
- **FLOPs**: Computational complexity analysis for efficiency assessment

### Results:
- Successfully implemented both U-shaped and latent-space DiffiT architectures
- Demonstrated effective image generation on CIFAR-10 and Imagenette datasets
- Achieved comprehensive evaluation with multiple complementary metrics
- Visualized the complete diffusion process from noise to realistic images
- Provided computational efficiency analysis through FLOPs calculation

### Evaluation Framework Benefits:
- **Multi-metric approach** provides comprehensive quality assessment
- **FID & KID** capture different aspects of distribution matching
- **LPIPS** measures perceptual quality from human vision perspective  
- **FLOPs** enables comparison of computational efficiency across models

### Future Work:
- Experiment with larger datasets and higher resolution images
- Implement conditional generation with class labels
- Explore different noise scheduling strategies (cosine, polynomial)
- Compare with other state-of-the-art diffusion models using same metrics
- Optimize model architecture for better FLOPs/quality trade-off
- Investigate the correlation between different evaluation metrics

### Model Comparison Guidelines:
When comparing with other diffusion models, consider:
1. **FID**: Primary metric for overall quality
2. **KID**: More reliable for small evaluation sets
3. **LPIPS**: Best for perceptual quality assessment
4. **FLOPs**: Critical for deployment and efficiency comparison

### References:
- DiffiT Paper: [Diffusion Vision Transformers for Image Generation](https://arxiv.org/abs/2312.02139)
- FID: [GANs Trained by a Two Time-Scale Update Rule](https://arxiv.org/abs/1706.08500)
- KID: [Demystifying MMD GANs](https://arxiv.org/abs/1801.01401)
- LPIPS: [The Unreasonable Effectiveness of Deep Features](https://arxiv.org/abs/1801.03924)
"""